{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook takes the raw data from the API and processes it to the modeling data.\n",
    "\n",
    "#### 2) Moving averages, percent changes, and z-scores are calcuated for the 21 ticker symbols. These are written to interim data as \"processed\" individual csv files.\n",
    "\n",
    "#### 3) Histograms were generated for means of z-scores. Four of the original 25 tickers were removed based on unusual z-scores or volume data issues. JNK was removed due to weirdly high open z-score mean, HYG was removed due to weirdly low low z-score mean, and EWZ/IEF had volume data isses (infinite end values). These values can be seen in a table <a href='https://docs.google.com/spreadsheets/d/1ekkdNC7Cg0UogQIs47jpgDZtKHj7IqdhR1BB1X-emIc/edit?usp=sharing'>here</a> and in a histogram <a href='https://github.com/trentleslie/springboard_final_capstone/blob/main/docs/all_low_z_means.png'>here</a>.\n",
    "\n",
    "#### 4) Rolling window processing is then conductecd on the individual processed csv files. For each rolling window size, the window is flattened to one row, added to a ticker dataframe, and written to its an individual csv file for each ticker's flattened data.\n",
    "#### 5) The flattened data sets for each ticker are then concatentated and written to a csv file for each window size in the final processed data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [20,320]\n",
    "\n",
    "def zscore(x, window):\n",
    "    r = x.rolling(window=window)\n",
    "    m = r.mean().shift(1)\n",
    "    s = r.std(ddof=0).shift(1)\n",
    "    z = (x-m)/s\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir('../data/raw/') if isfile(join('../data/raw/', f))]\n",
    "onlyfiles = list(filter(lambda thisfilename: '_daily.csv' in thisfilename, onlyfiles))\n",
    "\n",
    "ticker_list = []\n",
    "ticker_stats_mean = pd.DataFrame()\n",
    "ticker_stats_std = pd.DataFrame()\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    for filename in onlyfiles:\n",
    "        ticker = filename.split('_')[0]\n",
    "        ticker_list.append(ticker)\n",
    "        \n",
    "        # import csv, sort by date so percent change works, drop unneeded columns,\n",
    "        # rename columns, calculate moving averages, calulate percent changes, drop na's\n",
    "        df = pd.read_csv(f\"../data/raw/{ticker}_daily.csv\")\n",
    "        df.sort_index(inplace=True, ascending=False)\n",
    "        df = df.drop(['Unnamed: 0','5. adjusted close', '7. dividend amount', '8. split coefficient','SMA','EMA'], axis=1)\n",
    "        df.columns = ['open','high','low','close','volume','rsi']\n",
    "\n",
    "        # moving averages, convert to percentage of close\n",
    "        df['sma'] = df.iloc[:,3].rolling(window=50).mean()/df.iloc[:,3]\n",
    "        df['ema'] = df.iloc[:,3].ewm(span=21).mean()/df.iloc[:,3]\n",
    "\n",
    "        # percent change\n",
    "        df['open'] = df['open'].pct_change()\n",
    "        df['high'] = df['high'].pct_change()\n",
    "        df['low'] = df['low'].pct_change()\n",
    "        df['close'] = df['close'].pct_change()\n",
    "        df['volume'] = df['volume'].pct_change()\n",
    "        df = df.dropna()\n",
    "\n",
    "        # zscore\n",
    "        df['open'] =zscore(df['open'], window=window_size)\n",
    "        df['high'] = zscore(df['high'], window=window_size)\n",
    "        df['low'] = zscore(df['low'], window=window_size)\n",
    "        df['close'] = zscore(df['close'], window=window_size)\n",
    "        df['volume'] = zscore(df['volume'], window=window_size)\n",
    "        df['rsi'] = zscore(df['rsi'], window=window_size)\n",
    "        df['sma'] = zscore(df['sma'], window=window_size)\n",
    "        df['ema'] = zscore(df['ema'], window=window_size)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # ticker mean stats\n",
    "        temp = pd.DataFrame(df.describe()).iloc[1:2,]\n",
    "        temp['ticker'] = ticker\n",
    "        ticker_stats_mean = pd.concat([ticker_stats_mean, temp])\n",
    "        ticker_stats_mean.to_csv(f\"../data/interim/ticker_stats_mean_{window_size}.csv\")\n",
    "        \n",
    "        # ticker std stats\n",
    "        temp = pd.DataFrame(df.describe()).iloc[2:3,]\n",
    "        temp['ticker'] = ticker\n",
    "        ticker_stats_std = pd.concat([ticker_stats_std, temp])\n",
    "        ticker_stats_std.to_csv(f\"../data/interim/ticker_stats_std_{window_size}.csv\")\n",
    "\n",
    "        # write data and describe to csv\n",
    "        df.to_csv(f\"../data/interim/{ticker}_{window_size}_processed.csv\")\n",
    "        df.describe().to_csv(f\"../data/interim/{ticker}_{window_size}_describe.csv\")\n",
    "    \n",
    "    print(f\"{window_size} z-score mean histograms\")\n",
    "    ticker_stats_mean.hist(column='open', bins=10)\n",
    "    ticker_stats_mean.hist(column='high', bins=10)\n",
    "    ticker_stats_mean.hist(column='low', bins=10)\n",
    "    ticker_stats_mean.hist(column='close', bins=10)\n",
    "    ticker_stats_mean.hist(column='volume', bins=10)\n",
    "    ticker_stats_mean.hist(column='rsi', bins=10)\n",
    "    ticker_stats_mean.hist(column='sma', bins=10)\n",
    "    ticker_stats_mean.hist(column='ema', bins=10)\n",
    "\n",
    "    print(f\"{window_size} z-score std histograms\")\n",
    "    ticker_stats_std.hist(column='open', bins=10)\n",
    "    ticker_stats_std.hist(column='high', bins=10)\n",
    "    ticker_stats_std.hist(column='low', bins=10)\n",
    "    ticker_stats_std.hist(column='close', bins=10)\n",
    "    ticker_stats_std.hist(column='volume', bins=10)\n",
    "    ticker_stats_std.hist(column='rsi', bins=10)\n",
    "    ticker_stats_std.hist(column='sma', bins=10)\n",
    "    ticker_stats_std.hist(column='ema', bins=10)\n",
    "ticker_list = set(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting window size: 20\n",
      "BND 20 flattened\n",
      "EEM 20 flattened\n",
      "EFA 20 flattened\n",
      "FXI 20 flattened\n",
      "GDX 20 flattened\n",
      "IEF 20 flattened\n",
      "IEMG 20 flattened\n",
      "JETS 20 flattened\n",
      "LQD 20 flattened\n",
      "QQQ 20 flattened\n",
      "SLV 20 flattened\n",
      "SPY 20 flattened\n",
      "TLT 20 flattened\n",
      "VWO 20 flattened\n",
      "XLB 20 flattened\n",
      "XLE 20 flattened\n",
      "XLF 20 flattened\n",
      "XLI 20 flattened\n",
      "XLK 20 flattened\n",
      "XLU 20 flattened\n",
      "XLV 20 flattened\n",
      "window 20 data combined\n",
      "(196815, 156)\n",
      "starting window size: 320\n",
      "BND 320 flattened\n",
      "EEM 320 flattened\n",
      "EFA 320 flattened\n",
      "FXI 320 flattened\n",
      "GDX 320 flattened\n",
      "IEF 320 flattened\n",
      "IEMG 320 flattened\n",
      "JETS 320 flattened\n",
      "LQD 320 flattened\n",
      "QQQ 320 flattened\n",
      "SLV 320 flattened\n",
      "SPY 320 flattened\n",
      "TLT 320 flattened\n",
      "VWO 320 flattened\n",
      "XLB 320 flattened\n",
      "XLE 320 flattened\n",
      "XLF 320 flattened\n",
      "XLI 320 flattened\n",
      "XLK 320 flattened\n",
      "XLU 320 flattened\n",
      "XLV 320 flattened\n",
      "window 320 data combined\n",
      "(87157, 2556)\n"
     ]
    }
   ],
   "source": [
    "# iterating through the window sizes provided in the list above\n",
    "for window_size in window_sizes:\n",
    "    print(f\"starting window size: {window_size}\")\n",
    "    \n",
    "    # iterating through the list of processed files\n",
    "    for ticker in ticker_list:\n",
    "        # empty dataframe for this ticker's flattened data\n",
    "        flattened_df = pd.DataFrame()\n",
    "        df = pd.read_csv(f\"../data/interim/{ticker}_{window_size}_processed.csv\").drop(['Unnamed: 0'], axis=1)\n",
    "        \n",
    "        # iterating through grouped rows (window size)\n",
    "        for i in range(df.shape[0]-window_size+1):\n",
    "            # resetting the index each time so column names align\n",
    "            df_window = df.iloc[i:i+window_size,].reset_index(drop=True)\n",
    "            # mapping the index as a string so it can be concatenated\n",
    "            df_window.index = df_window.index.map(str)\n",
    "            # unstacking the window to one row\n",
    "            df_window = df_window.unstack().to_frame().sort_index(level=1).T\n",
    "            # renaming the columns\n",
    "            df_window.columns = df_window.columns.map('_'.join)\n",
    "            # concatenating the flattened row to the dataframe\n",
    "            flattened_df = pd.concat([flattened_df, df_window], axis=0)\n",
    "        \n",
    "        # writing the flattened dataframe for this ticker and window size to csv\n",
    "        flattened_df.to_csv(f\"../data/interim/{ticker}_{window_size}_flattened.csv\")\n",
    "        print(f\"{ticker} {window_size} flattened\")\n",
    "    \n",
    "    # getting list of flattened ticker files generated above for this window size\n",
    "    onlyfiles_flattened = [f for f in listdir('../data/interim/') if isfile(join('../data/interim/', f))]\n",
    "    onlyfiles_flattened = list(filter(lambda thisfilename: f\"{window_size}_flattened.csv\" in thisfilename, onlyfiles_flattened))\n",
    "    \n",
    "    # creating an empty dataframe for the combined flattened data\n",
    "    all_data_combined = pd.DataFrame()\n",
    "\n",
    "    # iterating through the list of flattened files for this window size and concatenating them to the dataframe\n",
    "    for filename in onlyfiles_flattened:\n",
    "        ticker = filename.split('_')[0]\n",
    "        all_data_combined = pd.concat([all_data_combined, pd.read_csv(f\"../data/interim/{ticker}_{window_size}_flattened.csv\").drop(['Unnamed: 0'], axis=1)])\n",
    "    \n",
    "    all_data_combined = all_data_combined.drop([f\"volume_{window_size-1}\", f\"rsi_{window_size-1}\", f\"sma_{window_size-1}\", f\"ema_{window_size-1}\"], axis=1)\n",
    "    \n",
    "    # writing the combined flattened dataframe for this window size to csv in the processed folder    \n",
    "    all_data_combined.to_csv(f\"../data/processed/all_processed_{window_size}.csv\")\n",
    "    print(f\"window {window_size} data combined\")\n",
    "    print(all_data_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XLV_daily.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#window 5: 96618 rows, 36 columns (32 features, 4 targets)\n",
    "#window 10: 96408 rows, 76 columns (72 features, 4 targets)\n",
    "#window 20: 95988 rows, 156 columns (152 features, 4 targets)\n",
    "\n",
    "# from 5 to 10, losing 10 rows per ticker, so with 21 tickers, 21 * 10 = 210 rows lost\n",
    "# from 10 to 20, losing an additional 20 rows per ticker, so 21 * 20 = 420 rows lost (630 total)\n",
    "filename"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4c1afcaa0824698e49fb009c9da9eaf010fa52d2eadd2a196450101a9336fb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('beta_lactamase': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
