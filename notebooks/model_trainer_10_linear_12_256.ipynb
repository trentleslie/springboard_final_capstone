{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import multiprocessing\n",
    "    \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    \n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False,layer_activation=\"linear\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=layer_activation))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67485 samples, validate on 28923 samples\n",
      "Epoch 1/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5895 - mean_absolute_error: 0.9714\n",
      "Epoch 00001: val_loss improved from inf to 0.59152, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-12-units-256-b.h5\n",
      "67485/67485 [==============================] - 79s 1ms/sample - loss: 0.5894 - mean_absolute_error: 0.9714 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 2/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00002: val_loss did not improve from 0.59152\n",
      "67485/67485 [==============================] - 52s 772us/sample - loss: 0.5893 - mean_absolute_error: 0.9713 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 3/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5841 - mean_absolute_error: 0.9652\n",
      "Epoch 00003: val_loss improved from 0.59152 to 0.57581, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-12-units-256-b.h5\n",
      "67485/67485 [==============================] - 53s 781us/sample - loss: 0.5840 - mean_absolute_error: 0.9651 - val_loss: 0.5758 - val_mean_absolute_error: 0.9552\n",
      "Epoch 4/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5734 - mean_absolute_error: 0.9522\n",
      "Epoch 00004: val_loss improved from 0.57581 to 0.57383, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-12-units-256-b.h5\n",
      "67485/67485 [==============================] - 52s 770us/sample - loss: 0.5733 - mean_absolute_error: 0.9521 - val_loss: 0.5738 - val_mean_absolute_error: 0.9531\n",
      "Epoch 5/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5742 - mean_absolute_error: 0.9534\n",
      "Epoch 00005: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 52s 765us/sample - loss: 0.5743 - mean_absolute_error: 0.9536 - val_loss: 0.5771 - val_mean_absolute_error: 0.9568\n",
      "Epoch 6/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5755 - mean_absolute_error: 0.9549\n",
      "Epoch 00006: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 52s 765us/sample - loss: 0.5755 - mean_absolute_error: 0.9548 - val_loss: 0.5770 - val_mean_absolute_error: 0.9564\n",
      "Epoch 7/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5775 - mean_absolute_error: 0.9570\n",
      "Epoch 00007: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 800us/sample - loss: 0.5775 - mean_absolute_error: 0.9570 - val_loss: 0.5800 - val_mean_absolute_error: 0.9599\n",
      "Epoch 8/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5789 - mean_absolute_error: 0.9588\n",
      "Epoch 00008: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 813us/sample - loss: 0.5789 - mean_absolute_error: 0.9588 - val_loss: 0.5797 - val_mean_absolute_error: 0.9599\n",
      "Epoch 9/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5819 - mean_absolute_error: 0.9623\n",
      "Epoch 00009: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 799us/sample - loss: 0.5820 - mean_absolute_error: 0.9624 - val_loss: 0.5908 - val_mean_absolute_error: 0.9724\n",
      "Epoch 10/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5886 - mean_absolute_error: 0.9705\n",
      "Epoch 00010: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 796us/sample - loss: 0.5885 - mean_absolute_error: 0.9703 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 11/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5884 - mean_absolute_error: 0.9702\n",
      "Epoch 00011: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 795us/sample - loss: 0.5884 - mean_absolute_error: 0.9701 - val_loss: 0.5905 - val_mean_absolute_error: 0.9723\n",
      "Epoch 12/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5868 - mean_absolute_error: 0.9684\n",
      "Epoch 00012: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 810us/sample - loss: 0.5868 - mean_absolute_error: 0.9684 - val_loss: 0.5853 - val_mean_absolute_error: 0.9665\n",
      "Epoch 13/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5829 - mean_absolute_error: 0.9637\n",
      "Epoch 00013: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 800us/sample - loss: 0.5829 - mean_absolute_error: 0.9638 - val_loss: 0.5844 - val_mean_absolute_error: 0.9655\n",
      "Epoch 14/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5813 - mean_absolute_error: 0.9619\n",
      "Epoch 00014: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 802us/sample - loss: 0.5813 - mean_absolute_error: 0.9619 - val_loss: 0.5834 - val_mean_absolute_error: 0.9641\n",
      "Epoch 15/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5823 - mean_absolute_error: 0.9631\n",
      "Epoch 00015: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 842us/sample - loss: 0.5823 - mean_absolute_error: 0.9631 - val_loss: 0.5851 - val_mean_absolute_error: 0.9661\n",
      "Epoch 16/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5822 - mean_absolute_error: 0.9630\n",
      "Epoch 00016: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 862us/sample - loss: 0.5823 - mean_absolute_error: 0.9630 - val_loss: 0.5856 - val_mean_absolute_error: 0.9670\n",
      "Epoch 17/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5809 - mean_absolute_error: 0.9612\n",
      "Epoch 00017: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5809 - mean_absolute_error: 0.9612 - val_loss: 0.5823 - val_mean_absolute_error: 0.9634\n",
      "Epoch 18/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5845 - mean_absolute_error: 0.9656\n",
      "Epoch 00018: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 862us/sample - loss: 0.5844 - mean_absolute_error: 0.9655 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 19/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00019: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 839us/sample - loss: 0.5894 - mean_absolute_error: 0.9713 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 20/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00020: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 848us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 21/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00021: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 849us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 22/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00022: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 56s 834us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5919 - val_mean_absolute_error: 0.9735\n",
      "Epoch 23/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00023: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 845us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 24/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00024: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 59s 869us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9714\n",
      "Epoch 00025: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 851us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5919 - val_mean_absolute_error: 0.9736\n",
      "Epoch 26/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00026: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 27/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00027: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 28/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00028: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 858us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9734\n",
      "Epoch 29/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9714\n",
      "Epoch 00029: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 865us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 30/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00030: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 31/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00031: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 856us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 32/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00032: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 857us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 33/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00033: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 59s 872us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 34/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00034: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 35/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00035: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 839us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 36/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00036: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 822us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 37/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00037: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 815us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 38/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00038: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 815us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 39/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00039: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 822us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 40/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00040: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 41/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713- ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.971\n",
      "Epoch 00041: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 845us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 42/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00042: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 844us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 43/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00043: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 56s 833us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9734\n",
      "Epoch 44/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00044: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 846us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 45/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00045: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 60s 887us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 46/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00046: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 47/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00047: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 48/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00048: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 49/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00049: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 863us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00050: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 51/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00051: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 862us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 52/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00052: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 863us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 53/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00053: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 857us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 54/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00054: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 864us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 55/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00055: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 858us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 56/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00056: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 864us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 57/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00057: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 58/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5895 - mean_absolute_error: 0.9714\n",
      "Epoch 00058: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 858us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 59/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00059: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 60/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00060: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 866us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 61/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00061: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 62/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00062: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 851us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 63/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00063: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 64/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00064: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 65/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00065: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 66/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00066: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 850us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 67/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00067: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 68/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9712\n",
      "Epoch 00068: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 849us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 69/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00069: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 850us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 70/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00070: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 71/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00071: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 72/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00072: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 856us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 73/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00073: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 74/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00074: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00075: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 845us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 76/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00076: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 840us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 77/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5891 - mean_absolute_error: 0.9710\n",
      "Epoch 00077: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 851us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 78/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5891 - mean_absolute_error: 0.9710\n",
      "Epoch 00078: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 862us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 79/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00079: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 857us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 80/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00080: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 81/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00081: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 848us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 82/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00082: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5918 - val_mean_absolute_error: 0.9735\n",
      "Epoch 83/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00083: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 854us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 84/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5890 - mean_absolute_error: 0.9709\n",
      "Epoch 00084: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 856us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 85/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00085: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 86/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00086: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 853us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 87/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00087: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 855us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n",
      "Epoch 88/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9713\n",
      "Epoch 00088: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 859us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 89/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9711\n",
      "Epoch 00089: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 854us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 90/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00090: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 857us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 91/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00091: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 852us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 92/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00092: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 854us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 93/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00093: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 57s 846us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9734\n",
      "Epoch 94/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00094: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 58s 857us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5919 - val_mean_absolute_error: 0.9735\n",
      "Epoch 95/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00095: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 56s 835us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5920 - val_mean_absolute_error: 0.9737\n",
      "Epoch 96/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5892 - mean_absolute_error: 0.9712\n",
      "Epoch 00096: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 809us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 97/100\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00097: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 53s 792us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 98/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.9713\n",
      "Epoch 00098: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 54s 800us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5917 - val_mean_absolute_error: 0.9734\n",
      "Epoch 99/100\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00099: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 821us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5916 - val_mean_absolute_error: 0.9733\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5893 - mean_absolute_error: 0.9712\n",
      "Epoch 00100: val_loss did not improve from 0.57383\n",
      "67485/67485 [==============================] - 55s 815us/sample - loss: 0.5893 - mean_absolute_error: 0.9712 - val_loss: 0.5915 - val_mean_absolute_error: 0.9733\n"
     ]
    }
   ],
   "source": [
    "#def run_tensorflow():\n",
    "\n",
    "# create these folders if they does not exist\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 72\n",
    "# Lookup step, 1 is the next day\n",
    "#LOOKUP_STEP = int(run_dict[run][\"LOOKUP_STEP\"])\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.3\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"close_0\",\"ema_0\",\"high_0\",\"low_0\",\"open_0\",\"rsi_0\",\"sma_0\",\"volume_0\",\"close_1\",\"ema_1\",\"high_1\",\"low_1\",\"open_1\",\"rsi_1\",\"sma_1\",\"volume_1\",\n",
    "                   \"close_2\",\"ema_2\",\"high_2\",\"low_2\",\"open_2\",\"rsi_2\",\"sma_2\",\"volume_2\",\"close_3\",\"ema_3\",\"high_3\",\"low_3\",\"open_3\",\"rsi_3\",\"sma_3\",\"volume_3\",\n",
    "                   \"close_4\",\"ema_4\",\"high_4\",\"low_4\",\"open_4\",\"rsi_4\",\"sma_4\",\"volume_4\",\"close_5\",\"ema_5\",\"high_5\",\"low_5\",\"open_5\",\"rsi_5\",\"sma_5\",\"volume_5\",\n",
    "                   \"close_6\",\"ema_6\",\"high_6\",\"low_6\",\"open_6\",\"rsi_6\",\"sma_6\",\"volume_6\",\"close_7\",\"ema_7\",\"high_7\",\"low_7\",\"open_7\",\"rsi_7\",\"sma_7\",\"volume_7\",\n",
    "                   \"close_8\",\"ema_8\",\"high_8\",\"low_8\",\"open_8\",\"rsi_8\",\"sma_8\",\"volume_8\"]\n",
    "TARGET_COLUMNS = [\"close_9\",\"high_9\",\"low_9\",\"open_9\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 12\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.3\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "LAYER_ACTIVATION = \"linear\"\n",
    "\n",
    "# Stock market\n",
    "ticker = \"MIXED\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-{LAYER_ACTIVATION}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#try:\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/processed/all_processed_10.csv')\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL, layer_activation=LAYER_ACTIVATION)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "X = data[FEATURE_COLUMNS]\n",
    "y = data[TARGET_COLUMNS]\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# reshape X to fit the neural network\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")\n",
    "\n",
    "#except:\n",
    "#    print(\"There was an attempt.\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
