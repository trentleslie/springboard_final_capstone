{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import multiprocessing\n",
    "    \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    \n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False,layer_activation=\"linear\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=layer_activation))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67485 samples, validate on 28923 samples\n",
      "Epoch 1/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5602 - mean_absolute_error: 0.9376\n",
      "Epoch 00001: val_loss improved from inf to 0.55696, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 22s 325us/sample - loss: 0.5603 - mean_absolute_error: 0.9377 - val_loss: 0.5570 - val_mean_absolute_error: 0.9329\n",
      "Epoch 2/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5542 - mean_absolute_error: 0.9305\n",
      "Epoch 00002: val_loss improved from 0.55696 to 0.55430, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.5545 - mean_absolute_error: 0.9307 - val_loss: 0.5543 - val_mean_absolute_error: 0.9298\n",
      "Epoch 3/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5518 - mean_absolute_error: 0.9274\n",
      "Epoch 00003: val_loss improved from 0.55430 to 0.55295, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.5516 - mean_absolute_error: 0.9273 - val_loss: 0.5529 - val_mean_absolute_error: 0.9282\n",
      "Epoch 4/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5490 - mean_absolute_error: 0.9240\n",
      "Epoch 00004: val_loss did not improve from 0.55295\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.5490 - mean_absolute_error: 0.9240 - val_loss: 0.5534 - val_mean_absolute_error: 0.9289\n",
      "Epoch 5/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5465 - mean_absolute_error: 0.9212\n",
      "Epoch 00005: val_loss improved from 0.55295 to 0.55120, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.5464 - mean_absolute_error: 0.9210 - val_loss: 0.5512 - val_mean_absolute_error: 0.9258\n",
      "Epoch 6/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5429 - mean_absolute_error: 0.9167\n",
      "Epoch 00006: val_loss improved from 0.55120 to 0.55061, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.5429 - mean_absolute_error: 0.9167 - val_loss: 0.5506 - val_mean_absolute_error: 0.9253\n",
      "Epoch 7/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5401 - mean_absolute_error: 0.9136\n",
      "Epoch 00007: val_loss did not improve from 0.55061\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.5402 - mean_absolute_error: 0.9137 - val_loss: 0.5512 - val_mean_absolute_error: 0.9261\n",
      "Epoch 8/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.5358 - mean_absolute_error: 0.9083\n",
      "Epoch 00008: val_loss improved from 0.55061 to 0.54729, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.5365 - mean_absolute_error: 0.9091 - val_loss: 0.5473 - val_mean_absolute_error: 0.9214\n",
      "Epoch 9/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5334 - mean_absolute_error: 0.9056\n",
      "Epoch 00009: val_loss improved from 0.54729 to 0.54561, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.5333 - mean_absolute_error: 0.9055 - val_loss: 0.5456 - val_mean_absolute_error: 0.9195\n",
      "Epoch 10/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5301 - mean_absolute_error: 0.9015\n",
      "Epoch 00010: val_loss did not improve from 0.54561\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.5300 - mean_absolute_error: 0.9014 - val_loss: 0.5457 - val_mean_absolute_error: 0.9198\n",
      "Epoch 11/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5269 - mean_absolute_error: 0.8975\n",
      "Epoch 00011: val_loss improved from 0.54561 to 0.54527, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.5269 - mean_absolute_error: 0.8976 - val_loss: 0.5453 - val_mean_absolute_error: 0.9189\n",
      "Epoch 12/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5229 - mean_absolute_error: 0.8929\n",
      "Epoch 00012: val_loss did not improve from 0.54527\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.5228 - mean_absolute_error: 0.8928 - val_loss: 0.5454 - val_mean_absolute_error: 0.9189\n",
      "Epoch 13/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5193 - mean_absolute_error: 0.8885\n",
      "Epoch 00013: val_loss improved from 0.54527 to 0.54338, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.5194 - mean_absolute_error: 0.8886 - val_loss: 0.5434 - val_mean_absolute_error: 0.9166\n",
      "Epoch 14/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5166 - mean_absolute_error: 0.8851\n",
      "Epoch 00014: val_loss improved from 0.54338 to 0.54333, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.5166 - mean_absolute_error: 0.8850 - val_loss: 0.5433 - val_mean_absolute_error: 0.9165\n",
      "Epoch 15/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5126 - mean_absolute_error: 0.8807\n",
      "Epoch 00015: val_loss improved from 0.54333 to 0.54195, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 255us/sample - loss: 0.5139 - mean_absolute_error: 0.8820 - val_loss: 0.5419 - val_mean_absolute_error: 0.9153\n",
      "Epoch 16/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5107 - mean_absolute_error: 0.8781\n",
      "Epoch 00016: val_loss did not improve from 0.54195\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.5108 - mean_absolute_error: 0.8782 - val_loss: 0.5424 - val_mean_absolute_error: 0.9155\n",
      "Epoch 17/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5083 - mean_absolute_error: 0.8752\n",
      "Epoch 00017: val_loss did not improve from 0.54195\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.5084 - mean_absolute_error: 0.8752 - val_loss: 0.5429 - val_mean_absolute_error: 0.9157\n",
      "Epoch 18/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5057 - mean_absolute_error: 0.8719\n",
      "Epoch 00018: val_loss improved from 0.54195 to 0.54084, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.5057 - mean_absolute_error: 0.8719 - val_loss: 0.5408 - val_mean_absolute_error: 0.9137\n",
      "Epoch 19/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5032 - mean_absolute_error: 0.8688\n",
      "Epoch 00019: val_loss did not improve from 0.54084\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.5032 - mean_absolute_error: 0.8688 - val_loss: 0.5414 - val_mean_absolute_error: 0.9144\n",
      "Epoch 20/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5005 - mean_absolute_error: 0.8656\n",
      "Epoch 00020: val_loss did not improve from 0.54084\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.5007 - mean_absolute_error: 0.8658 - val_loss: 0.5409 - val_mean_absolute_error: 0.9140\n",
      "Epoch 21/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4984 - mean_absolute_error: 0.8630\n",
      "Epoch 00021: val_loss improved from 0.54084 to 0.53949, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4986 - mean_absolute_error: 0.8632 - val_loss: 0.5395 - val_mean_absolute_error: 0.9119\n",
      "Epoch 22/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4970 - mean_absolute_error: 0.8614\n",
      "Epoch 00022: val_loss did not improve from 0.53949\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4970 - mean_absolute_error: 0.8614 - val_loss: 0.5407 - val_mean_absolute_error: 0.9136\n",
      "Epoch 23/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4953 - mean_absolute_error: 0.8593\n",
      "Epoch 00023: val_loss did not improve from 0.53949\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4952 - mean_absolute_error: 0.8592 - val_loss: 0.5398 - val_mean_absolute_error: 0.9125\n",
      "Epoch 24/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4932 - mean_absolute_error: 0.8568\n",
      "Epoch 00024: val_loss did not improve from 0.53949\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4932 - mean_absolute_error: 0.8568 - val_loss: 0.5398 - val_mean_absolute_error: 0.9125\n",
      "Epoch 25/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4913 - mean_absolute_error: 0.8540\n",
      "Epoch 00025: val_loss did not improve from 0.53949\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4913 - mean_absolute_error: 0.8540 - val_loss: 0.5409 - val_mean_absolute_error: 0.9134\n",
      "Epoch 26/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4891 - mean_absolute_error: 0.8515\n",
      "Epoch 00026: val_loss improved from 0.53949 to 0.53893, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4890 - mean_absolute_error: 0.8514 - val_loss: 0.5389 - val_mean_absolute_error: 0.9110\n",
      "Epoch 27/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4882 - mean_absolute_error: 0.8504\n",
      "Epoch 00027: val_loss did not improve from 0.53893\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4881 - mean_absolute_error: 0.8503 - val_loss: 0.5399 - val_mean_absolute_error: 0.9122\n",
      "Epoch 28/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4863 - mean_absolute_error: 0.8481\n",
      "Epoch 00028: val_loss did not improve from 0.53893\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4862 - mean_absolute_error: 0.8480 - val_loss: 0.5397 - val_mean_absolute_error: 0.9118\n",
      "Epoch 29/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4854 - mean_absolute_error: 0.8470\n",
      "Epoch 00029: val_loss did not improve from 0.53893\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4854 - mean_absolute_error: 0.8470 - val_loss: 0.5390 - val_mean_absolute_error: 0.9107\n",
      "Epoch 30/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4836 - mean_absolute_error: 0.8449\n",
      "Epoch 00030: val_loss improved from 0.53893 to 0.53889, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4838 - mean_absolute_error: 0.8450 - val_loss: 0.5389 - val_mean_absolute_error: 0.9107\n",
      "Epoch 31/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4827 - mean_absolute_error: 0.8434\n",
      "Epoch 00031: val_loss did not improve from 0.53889\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4828 - mean_absolute_error: 0.8436 - val_loss: 0.5391 - val_mean_absolute_error: 0.9111\n",
      "Epoch 32/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4815 - mean_absolute_error: 0.8420\n",
      "Epoch 00032: val_loss did not improve from 0.53889\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4815 - mean_absolute_error: 0.8421 - val_loss: 0.5401 - val_mean_absolute_error: 0.9124\n",
      "Epoch 33/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4800 - mean_absolute_error: 0.8401\n",
      "Epoch 00033: val_loss improved from 0.53889 to 0.53877, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4800 - mean_absolute_error: 0.8401 - val_loss: 0.5388 - val_mean_absolute_error: 0.9109\n",
      "Epoch 34/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4796 - mean_absolute_error: 0.8396\n",
      "Epoch 00034: val_loss did not improve from 0.53877\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4797 - mean_absolute_error: 0.8398 - val_loss: 0.5394 - val_mean_absolute_error: 0.9113\n",
      "Epoch 35/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4783 - mean_absolute_error: 0.8382\n",
      "Epoch 00035: val_loss improved from 0.53877 to 0.53834, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.4783 - mean_absolute_error: 0.8382 - val_loss: 0.5383 - val_mean_absolute_error: 0.9101\n",
      "Epoch 36/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4769 - mean_absolute_error: 0.8363\n",
      "Epoch 00036: val_loss did not improve from 0.53834\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4768 - mean_absolute_error: 0.8363 - val_loss: 0.5388 - val_mean_absolute_error: 0.9106\n",
      "Epoch 37/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4770 - mean_absolute_error: 0.8362\n",
      "Epoch 00037: val_loss improved from 0.53834 to 0.53643, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.4770 - mean_absolute_error: 0.8362 - val_loss: 0.5364 - val_mean_absolute_error: 0.9075\n",
      "Epoch 38/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4753 - mean_absolute_error: 0.8340\n",
      "Epoch 00038: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 17s 249us/sample - loss: 0.4753 - mean_absolute_error: 0.8341 - val_loss: 0.5372 - val_mean_absolute_error: 0.9086\n",
      "Epoch 39/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4742 - mean_absolute_error: 0.8326\n",
      "Epoch 00039: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4742 - mean_absolute_error: 0.8326 - val_loss: 0.5391 - val_mean_absolute_error: 0.9109\n",
      "Epoch 40/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4735 - mean_absolute_error: 0.8319\n",
      "Epoch 00040: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4736 - mean_absolute_error: 0.8319 - val_loss: 0.5379 - val_mean_absolute_error: 0.9095\n",
      "Epoch 41/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4729 - mean_absolute_error: 0.8312\n",
      "Epoch 00041: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4730 - mean_absolute_error: 0.8312 - val_loss: 0.5391 - val_mean_absolute_error: 0.9109\n",
      "Epoch 42/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4727 - mean_absolute_error: 0.8309\n",
      "Epoch 00042: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4725 - mean_absolute_error: 0.8306 - val_loss: 0.5397 - val_mean_absolute_error: 0.9118\n",
      "Epoch 43/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4711 - mean_absolute_error: 0.8289\n",
      "Epoch 00043: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4711 - mean_absolute_error: 0.8288 - val_loss: 0.5380 - val_mean_absolute_error: 0.9095\n",
      "Epoch 44/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4704 - mean_absolute_error: 0.8280\n",
      "Epoch 00044: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4703 - mean_absolute_error: 0.8280 - val_loss: 0.5381 - val_mean_absolute_error: 0.9100\n",
      "Epoch 45/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4706 - mean_absolute_error: 0.8279\n",
      "Epoch 00045: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 15s 229us/sample - loss: 0.4706 - mean_absolute_error: 0.8279 - val_loss: 0.5378 - val_mean_absolute_error: 0.9092\n",
      "Epoch 46/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4695 - mean_absolute_error: 0.8269\n",
      "Epoch 00046: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4695 - mean_absolute_error: 0.8269 - val_loss: 0.5376 - val_mean_absolute_error: 0.9091\n",
      "Epoch 47/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4696 - mean_absolute_error: 0.8266\n",
      "Epoch 00047: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4694 - mean_absolute_error: 0.8265 - val_loss: 0.5386 - val_mean_absolute_error: 0.9100\n",
      "Epoch 48/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4687 - mean_absolute_error: 0.8257\n",
      "Epoch 00048: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4686 - mean_absolute_error: 0.8257 - val_loss: 0.5378 - val_mean_absolute_error: 0.9093\n",
      "Epoch 49/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4681 - mean_absolute_error: 0.8247\n",
      "Epoch 00049: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 230us/sample - loss: 0.4682 - mean_absolute_error: 0.8249 - val_loss: 0.5381 - val_mean_absolute_error: 0.9098\n",
      "Epoch 50/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4681 - mean_absolute_error: 0.8251\n",
      "Epoch 00050: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4680 - mean_absolute_error: 0.8251 - val_loss: 0.5378 - val_mean_absolute_error: 0.9097\n",
      "Epoch 51/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4674 - mean_absolute_error: 0.8239\n",
      "Epoch 00051: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4674 - mean_absolute_error: 0.8239 - val_loss: 0.5381 - val_mean_absolute_error: 0.9095\n",
      "Epoch 52/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4661 - mean_absolute_error: 0.8223\n",
      "Epoch 00052: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4662 - mean_absolute_error: 0.8224 - val_loss: 0.5383 - val_mean_absolute_error: 0.9097\n",
      "Epoch 53/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4661 - mean_absolute_error: 0.8222\n",
      "Epoch 00053: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 15s 229us/sample - loss: 0.4661 - mean_absolute_error: 0.8222 - val_loss: 0.5378 - val_mean_absolute_error: 0.9093\n",
      "Epoch 54/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4656 - mean_absolute_error: 0.8221\n",
      "Epoch 00054: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.4656 - mean_absolute_error: 0.8220 - val_loss: 0.5390 - val_mean_absolute_error: 0.9106\n",
      "Epoch 55/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4645 - mean_absolute_error: 0.8204\n",
      "Epoch 00055: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4646 - mean_absolute_error: 0.8204 - val_loss: 0.5387 - val_mean_absolute_error: 0.9101\n",
      "Epoch 56/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4640 - mean_absolute_error: 0.8196\n",
      "Epoch 00056: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4640 - mean_absolute_error: 0.8196 - val_loss: 0.5382 - val_mean_absolute_error: 0.9097\n",
      "Epoch 57/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4644 - mean_absolute_error: 0.8200\n",
      "Epoch 00057: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4645 - mean_absolute_error: 0.8201 - val_loss: 0.5382 - val_mean_absolute_error: 0.9097\n",
      "Epoch 58/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4637 - mean_absolute_error: 0.8190\n",
      "Epoch 00058: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4636 - mean_absolute_error: 0.8190 - val_loss: 0.5372 - val_mean_absolute_error: 0.9086\n",
      "Epoch 59/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4640 - mean_absolute_error: 0.8194\n",
      "Epoch 00059: val_loss did not improve from 0.53643\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4638 - mean_absolute_error: 0.8192 - val_loss: 0.5379 - val_mean_absolute_error: 0.9093\n",
      "Epoch 60/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4633 - mean_absolute_error: 0.8186\n",
      "Epoch 00060: val_loss improved from 0.53643 to 0.53589, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4631 - mean_absolute_error: 0.8184 - val_loss: 0.5359 - val_mean_absolute_error: 0.9071\n",
      "Epoch 61/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4631 - mean_absolute_error: 0.8186\n",
      "Epoch 00061: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4631 - mean_absolute_error: 0.8186 - val_loss: 0.5360 - val_mean_absolute_error: 0.9070\n",
      "Epoch 62/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4619 - mean_absolute_error: 0.8168\n",
      "Epoch 00062: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4621 - mean_absolute_error: 0.8171 - val_loss: 0.5386 - val_mean_absolute_error: 0.9102\n",
      "Epoch 63/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4600 - mean_absolute_error: 0.8150\n",
      "Epoch 00063: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4621 - mean_absolute_error: 0.8171 - val_loss: 0.5366 - val_mean_absolute_error: 0.9075\n",
      "Epoch 64/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4612 - mean_absolute_error: 0.8159\n",
      "Epoch 00064: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4611 - mean_absolute_error: 0.8157 - val_loss: 0.5369 - val_mean_absolute_error: 0.9079\n",
      "Epoch 65/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4611 - mean_absolute_error: 0.8158\n",
      "Epoch 00065: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.4611 - mean_absolute_error: 0.8158 - val_loss: 0.5370 - val_mean_absolute_error: 0.9081\n",
      "Epoch 66/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4609 - mean_absolute_error: 0.8156\n",
      "Epoch 00066: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4609 - mean_absolute_error: 0.8156 - val_loss: 0.5386 - val_mean_absolute_error: 0.9102\n",
      "Epoch 67/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4608 - mean_absolute_error: 0.8155\n",
      "Epoch 00067: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.4608 - mean_absolute_error: 0.8155 - val_loss: 0.5371 - val_mean_absolute_error: 0.9081\n",
      "Epoch 68/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4602 - mean_absolute_error: 0.8144\n",
      "Epoch 00068: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 19s 275us/sample - loss: 0.4603 - mean_absolute_error: 0.8145 - val_loss: 0.5373 - val_mean_absolute_error: 0.9084\n",
      "Epoch 69/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4603 - mean_absolute_error: 0.8148\n",
      "Epoch 00069: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.4602 - mean_absolute_error: 0.8148 - val_loss: 0.5379 - val_mean_absolute_error: 0.9092\n",
      "Epoch 70/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4594 - mean_absolute_error: 0.8136\n",
      "Epoch 00070: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4594 - mean_absolute_error: 0.8136 - val_loss: 0.5377 - val_mean_absolute_error: 0.9090\n",
      "Epoch 71/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4593 - mean_absolute_error: 0.8133\n",
      "Epoch 00071: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4593 - mean_absolute_error: 0.8133 - val_loss: 0.5377 - val_mean_absolute_error: 0.9092\n",
      "Epoch 72/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4593 - mean_absolute_error: 0.8134\n",
      "Epoch 00072: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4593 - mean_absolute_error: 0.8134 - val_loss: 0.5368 - val_mean_absolute_error: 0.9080\n",
      "Epoch 73/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4589 - mean_absolute_error: 0.8131\n",
      "Epoch 00073: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4589 - mean_absolute_error: 0.8131 - val_loss: 0.5373 - val_mean_absolute_error: 0.9087\n",
      "Epoch 74/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4583 - mean_absolute_error: 0.8120\n",
      "Epoch 00074: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4585 - mean_absolute_error: 0.8123 - val_loss: 0.5381 - val_mean_absolute_error: 0.9094\n",
      "Epoch 75/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4584 - mean_absolute_error: 0.8123\n",
      "Epoch 00075: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4587 - mean_absolute_error: 0.8126 - val_loss: 0.5371 - val_mean_absolute_error: 0.9083\n",
      "Epoch 76/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4583 - mean_absolute_error: 0.8122\n",
      "Epoch 00076: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4584 - mean_absolute_error: 0.8122 - val_loss: 0.5364 - val_mean_absolute_error: 0.9074\n",
      "Epoch 77/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4576 - mean_absolute_error: 0.8113\n",
      "Epoch 00077: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4577 - mean_absolute_error: 0.8113 - val_loss: 0.5375 - val_mean_absolute_error: 0.9087\n",
      "Epoch 78/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4573 - mean_absolute_error: 0.8111\n",
      "Epoch 00078: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4574 - mean_absolute_error: 0.8111 - val_loss: 0.5377 - val_mean_absolute_error: 0.9090\n",
      "Epoch 79/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4570 - mean_absolute_error: 0.8103\n",
      "Epoch 00079: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4572 - mean_absolute_error: 0.8105 - val_loss: 0.5377 - val_mean_absolute_error: 0.9088\n",
      "Epoch 80/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4571 - mean_absolute_error: 0.8104\n",
      "Epoch 00080: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4572 - mean_absolute_error: 0.8105 - val_loss: 0.5381 - val_mean_absolute_error: 0.9093\n",
      "Epoch 81/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4568 - mean_absolute_error: 0.8105\n",
      "Epoch 00081: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4569 - mean_absolute_error: 0.8106 - val_loss: 0.5379 - val_mean_absolute_error: 0.9089\n",
      "Epoch 82/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4563 - mean_absolute_error: 0.8096\n",
      "Epoch 00082: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4563 - mean_absolute_error: 0.8096 - val_loss: 0.5388 - val_mean_absolute_error: 0.9102\n",
      "Epoch 83/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4558 - mean_absolute_error: 0.8090\n",
      "Epoch 00083: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4558 - mean_absolute_error: 0.8090 - val_loss: 0.5374 - val_mean_absolute_error: 0.9085\n",
      "Epoch 84/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4560 - mean_absolute_error: 0.8093\n",
      "Epoch 00084: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.4561 - mean_absolute_error: 0.8093 - val_loss: 0.5359 - val_mean_absolute_error: 0.9068\n",
      "Epoch 85/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4558 - mean_absolute_error: 0.8089\n",
      "Epoch 00085: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 249us/sample - loss: 0.4559 - mean_absolute_error: 0.8090 - val_loss: 0.5375 - val_mean_absolute_error: 0.9087\n",
      "Epoch 86/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4553 - mean_absolute_error: 0.8082\n",
      "Epoch 00086: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 252us/sample - loss: 0.4552 - mean_absolute_error: 0.8081 - val_loss: 0.5365 - val_mean_absolute_error: 0.9077\n",
      "Epoch 87/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4555 - mean_absolute_error: 0.8085\n",
      "Epoch 00087: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.4553 - mean_absolute_error: 0.8084 - val_loss: 0.5372 - val_mean_absolute_error: 0.9083\n",
      "Epoch 88/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4550 - mean_absolute_error: 0.8078\n",
      "Epoch 00088: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4550 - mean_absolute_error: 0.8077 - val_loss: 0.5369 - val_mean_absolute_error: 0.9079\n",
      "Epoch 89/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4553 - mean_absolute_error: 0.8082\n",
      "Epoch 00089: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4554 - mean_absolute_error: 0.8082 - val_loss: 0.5383 - val_mean_absolute_error: 0.9100\n",
      "Epoch 90/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4545 - mean_absolute_error: 0.8073\n",
      "Epoch 00090: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.4545 - mean_absolute_error: 0.8073 - val_loss: 0.5372 - val_mean_absolute_error: 0.9084\n",
      "Epoch 91/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4546 - mean_absolute_error: 0.8072\n",
      "Epoch 00091: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.4546 - mean_absolute_error: 0.8073 - val_loss: 0.5383 - val_mean_absolute_error: 0.9095\n",
      "Epoch 92/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4542 - mean_absolute_error: 0.8068\n",
      "Epoch 00092: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4545 - mean_absolute_error: 0.8071 - val_loss: 0.5376 - val_mean_absolute_error: 0.9088\n",
      "Epoch 93/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4539 - mean_absolute_error: 0.8064\n",
      "Epoch 00093: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4538 - mean_absolute_error: 0.8063 - val_loss: 0.5374 - val_mean_absolute_error: 0.9086\n",
      "Epoch 94/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4541 - mean_absolute_error: 0.8066\n",
      "Epoch 00094: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4543 - mean_absolute_error: 0.8069 - val_loss: 0.5376 - val_mean_absolute_error: 0.9089\n",
      "Epoch 95/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4533 - mean_absolute_error: 0.8056\n",
      "Epoch 00095: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4533 - mean_absolute_error: 0.8056 - val_loss: 0.5371 - val_mean_absolute_error: 0.9082\n",
      "Epoch 96/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4536 - mean_absolute_error: 0.8061\n",
      "Epoch 00096: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4536 - mean_absolute_error: 0.8061 - val_loss: 0.5365 - val_mean_absolute_error: 0.9076\n",
      "Epoch 97/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4536 - mean_absolute_error: 0.8059\n",
      "Epoch 00097: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4534 - mean_absolute_error: 0.8057 - val_loss: 0.5367 - val_mean_absolute_error: 0.9077\n",
      "Epoch 98/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4536 - mean_absolute_error: 0.8061\n",
      "Epoch 00098: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4535 - mean_absolute_error: 0.8059 - val_loss: 0.5375 - val_mean_absolute_error: 0.9086\n",
      "Epoch 99/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4527 - mean_absolute_error: 0.8049\n",
      "Epoch 00099: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4529 - mean_absolute_error: 0.8052 - val_loss: 0.5363 - val_mean_absolute_error: 0.9074\n",
      "Epoch 100/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4528 - mean_absolute_error: 0.8050\n",
      "Epoch 00100: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4529 - mean_absolute_error: 0.8051 - val_loss: 0.5373 - val_mean_absolute_error: 0.9086\n",
      "Epoch 101/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4532 - mean_absolute_error: 0.8056\n",
      "Epoch 00101: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4531 - mean_absolute_error: 0.8055 - val_loss: 0.5375 - val_mean_absolute_error: 0.9086\n",
      "Epoch 102/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4522 - mean_absolute_error: 0.8041\n",
      "Epoch 00102: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.4523 - mean_absolute_error: 0.8041 - val_loss: 0.5375 - val_mean_absolute_error: 0.9085\n",
      "Epoch 103/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4523 - mean_absolute_error: 0.8043\n",
      "Epoch 00103: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4523 - mean_absolute_error: 0.8043 - val_loss: 0.5369 - val_mean_absolute_error: 0.9078\n",
      "Epoch 104/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4519 - mean_absolute_error: 0.8037\n",
      "Epoch 00104: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 15s 229us/sample - loss: 0.4519 - mean_absolute_error: 0.8037 - val_loss: 0.5366 - val_mean_absolute_error: 0.9077\n",
      "Epoch 105/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4521 - mean_absolute_error: 0.8040\n",
      "Epoch 00105: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 15s 230us/sample - loss: 0.4521 - mean_absolute_error: 0.8040 - val_loss: 0.5369 - val_mean_absolute_error: 0.9079\n",
      "Epoch 106/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4503 - mean_absolute_error: 0.8022\n",
      "Epoch 00106: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4523 - mean_absolute_error: 0.8041 - val_loss: 0.5368 - val_mean_absolute_error: 0.9080\n",
      "Epoch 107/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4517 - mean_absolute_error: 0.8036\n",
      "Epoch 00107: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4516 - mean_absolute_error: 0.8035 - val_loss: 0.5366 - val_mean_absolute_error: 0.9077\n",
      "Epoch 108/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4519 - mean_absolute_error: 0.8037\n",
      "Epoch 00108: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4518 - mean_absolute_error: 0.8036 - val_loss: 0.5372 - val_mean_absolute_error: 0.9083\n",
      "Epoch 109/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4515 - mean_absolute_error: 0.8030\n",
      "Epoch 00109: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4515 - mean_absolute_error: 0.8030 - val_loss: 0.5370 - val_mean_absolute_error: 0.9080\n",
      "Epoch 110/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4518 - mean_absolute_error: 0.8035\n",
      "Epoch 00110: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4516 - mean_absolute_error: 0.8034 - val_loss: 0.5375 - val_mean_absolute_error: 0.9084\n",
      "Epoch 111/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4503 - mean_absolute_error: 0.8019\n",
      "Epoch 00111: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4507 - mean_absolute_error: 0.8023 - val_loss: 0.5370 - val_mean_absolute_error: 0.9077\n",
      "Epoch 112/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4515 - mean_absolute_error: 0.8031\n",
      "Epoch 00112: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4513 - mean_absolute_error: 0.8029 - val_loss: 0.5364 - val_mean_absolute_error: 0.9070\n",
      "Epoch 113/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4510 - mean_absolute_error: 0.8024\n",
      "Epoch 00113: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4510 - mean_absolute_error: 0.8024 - val_loss: 0.5371 - val_mean_absolute_error: 0.9078\n",
      "Epoch 114/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4508 - mean_absolute_error: 0.8023\n",
      "Epoch 00114: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.4507 - mean_absolute_error: 0.8022 - val_loss: 0.5365 - val_mean_absolute_error: 0.9073\n",
      "Epoch 115/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4502 - mean_absolute_error: 0.8017\n",
      "Epoch 00115: val_loss did not improve from 0.53589\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4505 - mean_absolute_error: 0.8021 - val_loss: 0.5377 - val_mean_absolute_error: 0.9085\n",
      "Epoch 116/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4505 - mean_absolute_error: 0.8017\n",
      "Epoch 00116: val_loss improved from 0.53589 to 0.53574, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4505 - mean_absolute_error: 0.8017 - val_loss: 0.5357 - val_mean_absolute_error: 0.9065\n",
      "Epoch 117/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4502 - mean_absolute_error: 0.8016\n",
      "Epoch 00117: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4502 - mean_absolute_error: 0.8015 - val_loss: 0.5357 - val_mean_absolute_error: 0.9064\n",
      "Epoch 118/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4502 - mean_absolute_error: 0.8016\n",
      "Epoch 00118: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4502 - mean_absolute_error: 0.8016 - val_loss: 0.5367 - val_mean_absolute_error: 0.9075\n",
      "Epoch 119/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4504 - mean_absolute_error: 0.8016\n",
      "Epoch 00119: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 230us/sample - loss: 0.4504 - mean_absolute_error: 0.8015 - val_loss: 0.5371 - val_mean_absolute_error: 0.9080\n",
      "Epoch 120/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4502 - mean_absolute_error: 0.8016\n",
      "Epoch 00120: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4502 - mean_absolute_error: 0.8016 - val_loss: 0.5365 - val_mean_absolute_error: 0.9073\n",
      "Epoch 121/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4496 - mean_absolute_error: 0.8008\n",
      "Epoch 00121: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4495 - mean_absolute_error: 0.8007 - val_loss: 0.5370 - val_mean_absolute_error: 0.9079\n",
      "Epoch 122/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4500 - mean_absolute_error: 0.8008\n",
      "Epoch 00122: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4500 - mean_absolute_error: 0.8007 - val_loss: 0.5358 - val_mean_absolute_error: 0.9064\n",
      "Epoch 123/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4499 - mean_absolute_error: 0.8013\n",
      "Epoch 00123: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4498 - mean_absolute_error: 0.8012 - val_loss: 0.5368 - val_mean_absolute_error: 0.9074\n",
      "Epoch 124/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4503 - mean_absolute_error: 0.8017\n",
      "Epoch 00124: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4502 - mean_absolute_error: 0.8016 - val_loss: 0.5367 - val_mean_absolute_error: 0.9074\n",
      "Epoch 125/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4495 - mean_absolute_error: 0.8006\n",
      "Epoch 00125: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.4496 - mean_absolute_error: 0.8006 - val_loss: 0.5379 - val_mean_absolute_error: 0.9087\n",
      "Epoch 126/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4491 - mean_absolute_error: 0.8000\n",
      "Epoch 00126: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4490 - mean_absolute_error: 0.7999 - val_loss: 0.5370 - val_mean_absolute_error: 0.9077\n",
      "Epoch 127/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4490 - mean_absolute_error: 0.7999\n",
      "Epoch 00127: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4490 - mean_absolute_error: 0.8000 - val_loss: 0.5359 - val_mean_absolute_error: 0.9067\n",
      "Epoch 128/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4487 - mean_absolute_error: 0.7998\n",
      "Epoch 00128: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4488 - mean_absolute_error: 0.7999 - val_loss: 0.5367 - val_mean_absolute_error: 0.9074\n",
      "Epoch 129/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4493 - mean_absolute_error: 0.8003\n",
      "Epoch 00129: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4493 - mean_absolute_error: 0.8003 - val_loss: 0.5360 - val_mean_absolute_error: 0.9067\n",
      "Epoch 130/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4485 - mean_absolute_error: 0.7992\n",
      "Epoch 00130: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4485 - mean_absolute_error: 0.7991 - val_loss: 0.5369 - val_mean_absolute_error: 0.9079\n",
      "Epoch 131/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4485 - mean_absolute_error: 0.7990\n",
      "Epoch 00131: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4484 - mean_absolute_error: 0.7989 - val_loss: 0.5370 - val_mean_absolute_error: 0.9080\n",
      "Epoch 132/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4490 - mean_absolute_error: 0.7996\n",
      "Epoch 00132: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4488 - mean_absolute_error: 0.7994 - val_loss: 0.5374 - val_mean_absolute_error: 0.9083\n",
      "Epoch 133/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4483 - mean_absolute_error: 0.7989\n",
      "Epoch 00133: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4484 - mean_absolute_error: 0.7989 - val_loss: 0.5360 - val_mean_absolute_error: 0.9067\n",
      "Epoch 134/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4480 - mean_absolute_error: 0.7988\n",
      "Epoch 00134: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4481 - mean_absolute_error: 0.7988 - val_loss: 0.5358 - val_mean_absolute_error: 0.9064\n",
      "Epoch 135/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4480 - mean_absolute_error: 0.7986\n",
      "Epoch 00135: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4480 - mean_absolute_error: 0.7986 - val_loss: 0.5363 - val_mean_absolute_error: 0.9070\n",
      "Epoch 136/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4483 - mean_absolute_error: 0.7991\n",
      "Epoch 00136: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4483 - mean_absolute_error: 0.7991 - val_loss: 0.5366 - val_mean_absolute_error: 0.9072\n",
      "Epoch 137/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4482 - mean_absolute_error: 0.7988\n",
      "Epoch 00137: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4484 - mean_absolute_error: 0.7991 - val_loss: 0.5377 - val_mean_absolute_error: 0.9088\n",
      "Epoch 138/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4480 - mean_absolute_error: 0.7988\n",
      "Epoch 00138: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4480 - mean_absolute_error: 0.7987 - val_loss: 0.5369 - val_mean_absolute_error: 0.9077\n",
      "Epoch 139/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4477 - mean_absolute_error: 0.7981\n",
      "Epoch 00139: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 223us/sample - loss: 0.4477 - mean_absolute_error: 0.7981 - val_loss: 0.5371 - val_mean_absolute_error: 0.9079\n",
      "Epoch 140/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4476 - mean_absolute_error: 0.7979\n",
      "Epoch 00140: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 220us/sample - loss: 0.4476 - mean_absolute_error: 0.7980 - val_loss: 0.5372 - val_mean_absolute_error: 0.9082\n",
      "Epoch 141/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4480 - mean_absolute_error: 0.7985\n",
      "Epoch 00141: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 218us/sample - loss: 0.4479 - mean_absolute_error: 0.7985 - val_loss: 0.5369 - val_mean_absolute_error: 0.9077\n",
      "Epoch 142/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4476 - mean_absolute_error: 0.7982\n",
      "Epoch 00142: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4475 - mean_absolute_error: 0.7981 - val_loss: 0.5367 - val_mean_absolute_error: 0.9075\n",
      "Epoch 143/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4472 - mean_absolute_error: 0.7974\n",
      "Epoch 00143: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 220us/sample - loss: 0.4472 - mean_absolute_error: 0.7974 - val_loss: 0.5371 - val_mean_absolute_error: 0.9079\n",
      "Epoch 144/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4470 - mean_absolute_error: 0.7971\n",
      "Epoch 00144: val_loss did not improve from 0.53574\n",
      "67485/67485 [==============================] - 15s 220us/sample - loss: 0.4470 - mean_absolute_error: 0.7971 - val_loss: 0.5368 - val_mean_absolute_error: 0.9076\n",
      "Epoch 145/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4473 - mean_absolute_error: 0.7975\n",
      "Epoch 00145: val_loss improved from 0.53574 to 0.53569, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 256us/sample - loss: 0.4473 - mean_absolute_error: 0.7975 - val_loss: 0.5357 - val_mean_absolute_error: 0.9063\n",
      "Epoch 146/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4464 - mean_absolute_error: 0.7965\n",
      "Epoch 00146: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4465 - mean_absolute_error: 0.7966 - val_loss: 0.5358 - val_mean_absolute_error: 0.9063\n",
      "Epoch 147/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4470 - mean_absolute_error: 0.7971\n",
      "Epoch 00147: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 17s 247us/sample - loss: 0.4470 - mean_absolute_error: 0.7970 - val_loss: 0.5365 - val_mean_absolute_error: 0.9070\n",
      "Epoch 148/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4470 - mean_absolute_error: 0.7972\n",
      "Epoch 00148: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4470 - mean_absolute_error: 0.7972 - val_loss: 0.5367 - val_mean_absolute_error: 0.9076\n",
      "Epoch 149/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4473 - mean_absolute_error: 0.7977\n",
      "Epoch 00149: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4473 - mean_absolute_error: 0.7977 - val_loss: 0.5377 - val_mean_absolute_error: 0.9084\n",
      "Epoch 150/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4467 - mean_absolute_error: 0.7967\n",
      "Epoch 00150: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4467 - mean_absolute_error: 0.7967 - val_loss: 0.5365 - val_mean_absolute_error: 0.9071\n",
      "Epoch 151/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4464 - mean_absolute_error: 0.7962\n",
      "Epoch 00151: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4464 - mean_absolute_error: 0.7962 - val_loss: 0.5363 - val_mean_absolute_error: 0.9068\n",
      "Epoch 152/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4466 - mean_absolute_error: 0.7966\n",
      "Epoch 00152: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 241us/sample - loss: 0.4466 - mean_absolute_error: 0.7966 - val_loss: 0.5375 - val_mean_absolute_error: 0.9083\n",
      "Epoch 153/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4463 - mean_absolute_error: 0.7965\n",
      "Epoch 00153: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4464 - mean_absolute_error: 0.7966 - val_loss: 0.5364 - val_mean_absolute_error: 0.9066\n",
      "Epoch 154/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4468 - mean_absolute_error: 0.7972\n",
      "Epoch 00154: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4468 - mean_absolute_error: 0.7971 - val_loss: 0.5365 - val_mean_absolute_error: 0.9070\n",
      "Epoch 155/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4465 - mean_absolute_error: 0.7965\n",
      "Epoch 00155: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4466 - mean_absolute_error: 0.7966 - val_loss: 0.5359 - val_mean_absolute_error: 0.9062\n",
      "Epoch 156/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4459 - mean_absolute_error: 0.7958\n",
      "Epoch 00156: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4459 - mean_absolute_error: 0.7958 - val_loss: 0.5372 - val_mean_absolute_error: 0.9077\n",
      "Epoch 157/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4464 - mean_absolute_error: 0.7965\n",
      "Epoch 00157: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.4464 - mean_absolute_error: 0.7965 - val_loss: 0.5359 - val_mean_absolute_error: 0.9062\n",
      "Epoch 158/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4461 - mean_absolute_error: 0.7959\n",
      "Epoch 00158: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.4461 - mean_absolute_error: 0.7959 - val_loss: 0.5359 - val_mean_absolute_error: 0.9064\n",
      "Epoch 159/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4460 - mean_absolute_error: 0.7958\n",
      "Epoch 00159: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 15s 230us/sample - loss: 0.4460 - mean_absolute_error: 0.7958 - val_loss: 0.5358 - val_mean_absolute_error: 0.9061\n",
      "Epoch 160/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4455 - mean_absolute_error: 0.7953\n",
      "Epoch 00160: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4456 - mean_absolute_error: 0.7955 - val_loss: 0.5365 - val_mean_absolute_error: 0.9071\n",
      "Epoch 161/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4458 - mean_absolute_error: 0.7957\n",
      "Epoch 00161: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4460 - mean_absolute_error: 0.7959 - val_loss: 0.5365 - val_mean_absolute_error: 0.9071\n",
      "Epoch 162/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4462 - mean_absolute_error: 0.7962\n",
      "Epoch 00162: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4460 - mean_absolute_error: 0.7960 - val_loss: 0.5360 - val_mean_absolute_error: 0.9065\n",
      "Epoch 163/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4459 - mean_absolute_error: 0.7958\n",
      "Epoch 00163: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 15s 228us/sample - loss: 0.4459 - mean_absolute_error: 0.7958 - val_loss: 0.5363 - val_mean_absolute_error: 0.9068\n",
      "Epoch 164/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4455 - mean_absolute_error: 0.7953\n",
      "Epoch 00164: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4456 - mean_absolute_error: 0.7954 - val_loss: 0.5366 - val_mean_absolute_error: 0.9071\n",
      "Epoch 165/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4457 - mean_absolute_error: 0.7956\n",
      "Epoch 00165: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 232us/sample - loss: 0.4457 - mean_absolute_error: 0.7956 - val_loss: 0.5364 - val_mean_absolute_error: 0.9070\n",
      "Epoch 166/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4452 - mean_absolute_error: 0.7948\n",
      "Epoch 00166: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4453 - mean_absolute_error: 0.7950 - val_loss: 0.5367 - val_mean_absolute_error: 0.9073\n",
      "Epoch 167/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4455 - mean_absolute_error: 0.7953\n",
      "Epoch 00167: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 17s 248us/sample - loss: 0.4455 - mean_absolute_error: 0.7953 - val_loss: 0.5368 - val_mean_absolute_error: 0.9076\n",
      "Epoch 168/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4453 - mean_absolute_error: 0.7948\n",
      "Epoch 00168: val_loss did not improve from 0.53569\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4454 - mean_absolute_error: 0.7949 - val_loss: 0.5362 - val_mean_absolute_error: 0.9070\n",
      "Epoch 169/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4456 - mean_absolute_error: 0.7952\n",
      "Epoch 00169: val_loss improved from 0.53569 to 0.53500, saving model to results\\2021-11-29_MIXED-huber_loss-adam-LSTM-relu--layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 15s 227us/sample - loss: 0.4454 - mean_absolute_error: 0.7950 - val_loss: 0.5350 - val_mean_absolute_error: 0.9053\n",
      "Epoch 170/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4452 - mean_absolute_error: 0.7949\n",
      "Epoch 00170: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4453 - mean_absolute_error: 0.7949 - val_loss: 0.5359 - val_mean_absolute_error: 0.9067\n",
      "Epoch 171/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4452 - mean_absolute_error: 0.7948\n",
      "Epoch 00171: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.4451 - mean_absolute_error: 0.7947 - val_loss: 0.5373 - val_mean_absolute_error: 0.9080\n",
      "Epoch 172/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4450 - mean_absolute_error: 0.7945\n",
      "Epoch 00172: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 229us/sample - loss: 0.4451 - mean_absolute_error: 0.7946 - val_loss: 0.5371 - val_mean_absolute_error: 0.9079\n",
      "Epoch 173/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4455 - mean_absolute_error: 0.7952\n",
      "Epoch 00173: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4454 - mean_absolute_error: 0.7951 - val_loss: 0.5359 - val_mean_absolute_error: 0.9066\n",
      "Epoch 174/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4449 - mean_absolute_error: 0.7943\n",
      "Epoch 00174: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.4449 - mean_absolute_error: 0.7943 - val_loss: 0.5363 - val_mean_absolute_error: 0.9068\n",
      "Epoch 175/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4452 - mean_absolute_error: 0.7949\n",
      "Epoch 00175: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4452 - mean_absolute_error: 0.7949 - val_loss: 0.5354 - val_mean_absolute_error: 0.9059\n",
      "Epoch 176/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4453 - mean_absolute_error: 0.7950\n",
      "Epoch 00176: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4453 - mean_absolute_error: 0.7950 - val_loss: 0.5353 - val_mean_absolute_error: 0.9056\n",
      "Epoch 177/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4448 - mean_absolute_error: 0.7944\n",
      "Epoch 00177: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 230us/sample - loss: 0.4449 - mean_absolute_error: 0.7946 - val_loss: 0.5365 - val_mean_absolute_error: 0.9070\n",
      "Epoch 178/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4451 - mean_absolute_error: 0.7945\n",
      "Epoch 00178: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4451 - mean_absolute_error: 0.7945 - val_loss: 0.5367 - val_mean_absolute_error: 0.9072\n",
      "Epoch 179/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4448 - mean_absolute_error: 0.7942\n",
      "Epoch 00179: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 234us/sample - loss: 0.4448 - mean_absolute_error: 0.7943 - val_loss: 0.5362 - val_mean_absolute_error: 0.9068\n",
      "Epoch 180/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4447 - mean_absolute_error: 0.7939\n",
      "Epoch 00180: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 228us/sample - loss: 0.4446 - mean_absolute_error: 0.7939 - val_loss: 0.5360 - val_mean_absolute_error: 0.9067\n",
      "Epoch 181/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4450 - mean_absolute_error: 0.7946\n",
      "Epoch 00181: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4449 - mean_absolute_error: 0.7945 - val_loss: 0.5358 - val_mean_absolute_error: 0.9064\n",
      "Epoch 182/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4446 - mean_absolute_error: 0.7940\n",
      "Epoch 00182: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 235us/sample - loss: 0.4446 - mean_absolute_error: 0.7940 - val_loss: 0.5358 - val_mean_absolute_error: 0.9066\n",
      "Epoch 183/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4443 - mean_absolute_error: 0.7939\n",
      "Epoch 00183: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4443 - mean_absolute_error: 0.7940 - val_loss: 0.5363 - val_mean_absolute_error: 0.9073\n",
      "Epoch 184/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4447 - mean_absolute_error: 0.7943\n",
      "Epoch 00184: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4446 - mean_absolute_error: 0.7941 - val_loss: 0.5368 - val_mean_absolute_error: 0.9077\n",
      "Epoch 185/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4444 - mean_absolute_error: 0.7936\n",
      "Epoch 00185: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 233us/sample - loss: 0.4445 - mean_absolute_error: 0.7938 - val_loss: 0.5357 - val_mean_absolute_error: 0.9062\n",
      "Epoch 186/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4444 - mean_absolute_error: 0.7935\n",
      "Epoch 00186: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.4444 - mean_absolute_error: 0.7935 - val_loss: 0.5363 - val_mean_absolute_error: 0.9067\n",
      "Epoch 187/200\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4422 - mean_absolute_error: 0.7911\n",
      "Epoch 00187: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4442 - mean_absolute_error: 0.7932 - val_loss: 0.5354 - val_mean_absolute_error: 0.9058\n",
      "Epoch 188/200\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4445 - mean_absolute_error: 0.7941\n",
      "Epoch 00188: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 236us/sample - loss: 0.4444 - mean_absolute_error: 0.7939 - val_loss: 0.5361 - val_mean_absolute_error: 0.9065\n",
      "Epoch 189/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4444 - mean_absolute_error: 0.7935\n",
      "Epoch 00189: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 231us/sample - loss: 0.4443 - mean_absolute_error: 0.7934 - val_loss: 0.5353 - val_mean_absolute_error: 0.9054\n",
      "Epoch 190/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4444 - mean_absolute_error: 0.7938\n",
      "Epoch 00190: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 225us/sample - loss: 0.4444 - mean_absolute_error: 0.7938 - val_loss: 0.5365 - val_mean_absolute_error: 0.9072\n",
      "Epoch 191/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4438 - mean_absolute_error: 0.7928\n",
      "Epoch 00191: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 227us/sample - loss: 0.4436 - mean_absolute_error: 0.7926 - val_loss: 0.5372 - val_mean_absolute_error: 0.9080\n",
      "Epoch 192/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4437 - mean_absolute_error: 0.7928\n",
      "Epoch 00192: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4436 - mean_absolute_error: 0.7928 - val_loss: 0.5356 - val_mean_absolute_error: 0.9061\n",
      "Epoch 193/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4441 - mean_absolute_error: 0.7936\n",
      "Epoch 00193: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4441 - mean_absolute_error: 0.7935 - val_loss: 0.5357 - val_mean_absolute_error: 0.9062\n",
      "Epoch 194/200\n",
      "67200/67485 [============================>.] - ETA: 0s - loss: 0.4442 - mean_absolute_error: 0.7935\n",
      "Epoch 00194: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 227us/sample - loss: 0.4438 - mean_absolute_error: 0.7931 - val_loss: 0.5356 - val_mean_absolute_error: 0.9059\n",
      "Epoch 195/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4439 - mean_absolute_error: 0.7929\n",
      "Epoch 00195: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 227us/sample - loss: 0.4438 - mean_absolute_error: 0.7929 - val_loss: 0.5365 - val_mean_absolute_error: 0.9072\n",
      "Epoch 196/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4440 - mean_absolute_error: 0.7932\n",
      "Epoch 00196: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4440 - mean_absolute_error: 0.7932 - val_loss: 0.5378 - val_mean_absolute_error: 0.9083\n",
      "Epoch 197/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4437 - mean_absolute_error: 0.7927\n",
      "Epoch 00197: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 227us/sample - loss: 0.4437 - mean_absolute_error: 0.7926 - val_loss: 0.5350 - val_mean_absolute_error: 0.9055\n",
      "Epoch 198/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4435 - mean_absolute_error: 0.7926\n",
      "Epoch 00198: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4434 - mean_absolute_error: 0.7925 - val_loss: 0.5360 - val_mean_absolute_error: 0.9065\n",
      "Epoch 199/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4436 - mean_absolute_error: 0.7928\n",
      "Epoch 00199: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4436 - mean_absolute_error: 0.7928 - val_loss: 0.5364 - val_mean_absolute_error: 0.9068\n",
      "Epoch 200/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4437 - mean_absolute_error: 0.7929\n",
      "Epoch 00200: val_loss did not improve from 0.53500\n",
      "67485/67485 [==============================] - 15s 226us/sample - loss: 0.4439 - mean_absolute_error: 0.7930 - val_loss: 0.5357 - val_mean_absolute_error: 0.9060\n"
     ]
    }
   ],
   "source": [
    "#def run_tensorflow():\n",
    "\n",
    "# create these folders if they does not exist\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 72\n",
    "# Lookup step, 1 is the next day\n",
    "#LOOKUP_STEP = int(run_dict[run][\"LOOKUP_STEP\"])\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.3\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"close_0\",\"ema_0\",\"high_0\",\"low_0\",\"open_0\",\"rsi_0\",\"sma_0\",\"volume_0\",\"close_1\",\"ema_1\",\"high_1\",\"low_1\",\"open_1\",\"rsi_1\",\"sma_1\",\"volume_1\",\n",
    "                   \"close_2\",\"ema_2\",\"high_2\",\"low_2\",\"open_2\",\"rsi_2\",\"sma_2\",\"volume_2\",\"close_3\",\"ema_3\",\"high_3\",\"low_3\",\"open_3\",\"rsi_3\",\"sma_3\",\"volume_3\",\n",
    "                   \"close_4\",\"ema_4\",\"high_4\",\"low_4\",\"open_4\",\"rsi_4\",\"sma_4\",\"volume_4\",\"close_5\",\"ema_5\",\"high_5\",\"low_5\",\"open_5\",\"rsi_5\",\"sma_5\",\"volume_5\",\n",
    "                   \"close_6\",\"ema_6\",\"high_6\",\"low_6\",\"open_6\",\"rsi_6\",\"sma_6\",\"volume_6\",\"close_7\",\"ema_7\",\"high_7\",\"low_7\",\"open_7\",\"rsi_7\",\"sma_7\",\"volume_7\",\n",
    "                   \"close_8\",\"ema_8\",\"high_8\",\"low_8\",\"open_8\",\"rsi_8\",\"sma_8\",\"volume_8\"]\n",
    "TARGET_COLUMNS = [\"close_9\",\"high_9\",\"low_9\",\"open_9\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.3\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "\n",
    "LAYER_ACTIVATION = \"relu\"\n",
    "\n",
    "# Stock market\n",
    "ticker = \"MIXED\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-{LAYER_ACTIVATION}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#try:\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/processed/all_processed_10.csv')\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL, layer_activation=LAYER_ACTIVATION)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "X = data[FEATURE_COLUMNS]\n",
    "y = data[TARGET_COLUMNS]\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# reshape X to fit the neural network\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")\n",
    "\n",
    "#except:\n",
    "#    print(\"There was an attempt.\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
