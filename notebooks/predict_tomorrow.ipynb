{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir      \n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take about 19.0 minutes to run.\n",
      "SPY saved\n",
      "QQQ saved\n",
      "XLF saved\n",
      "EEM saved\n",
      "XLE saved\n",
      "SLV saved\n",
      "FXI saved\n",
      "GDX saved\n",
      "EFA saved\n",
      "TLT saved\n",
      "LQD saved\n",
      "XLU saved\n",
      "XLV saved\n",
      "XLI saved\n",
      "XLK saved\n",
      "IEF saved\n",
      "XLB saved\n",
      "JETS saved\n",
      "BND saved\n"
     ]
    }
   ],
   "source": [
    "# symbols and technical indicators [code, interval, name]\n",
    "# https://www.alphavantage.co/documentation/#technical-indicators\n",
    "#\n",
    "# got rid of JNK (weirdly high open z-score mean), HYG (weirdly low low z-score mean), and EWZ/IEF (infinite end values)\n",
    "\n",
    "symbol_list = ['SPY','QQQ','XLF','EEM','XLE','SLV','FXI','GDX','EFA','TLT','LQD','XLU','XLV','XLI','XLK','IEF','XLB','JETS','BND']\n",
    "#symbol_list = ['SPY','QQQ']\n",
    "tech_list = [['SMA',50,'Technical Analysis: SMA'],\n",
    "             ['EMA',21,'Technical Analysis: EMA'],\n",
    "             ['RSI',14,'Technical Analysis: RSI']]\n",
    "\n",
    "print(f\"This will take about {len(symbol_list) * (len(tech_list) + 1)*15/60} minutes to run.\")\n",
    "\n",
    "for symbol in symbol_list:\n",
    "\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&outputsize=full&apikey=PDS8Y8E8KULJVDET\"\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    df_price = pd.DataFrame(data['Time Series (Daily)']).T\n",
    "    time.sleep(15)\n",
    "\n",
    "    for tech in tech_list:\n",
    "        url = f\"https://www.alphavantage.co/query?function={tech[0]}&symbol={symbol}&interval=daily&time_period={tech[1]}&series_type=close&apikey=PDS8Y8E8KULJVDET\"\n",
    "        r = requests.get(url)\n",
    "        data = r.json()\n",
    "        df_tech = pd.DataFrame(data[tech[2]]).T\n",
    "        df_price = df_price.merge(df_tech, how='inner', left_index=True, right_index=True)\n",
    "        time.sleep(15)\n",
    "    \n",
    "    df_price.to_csv(f\"../data/predict/{symbol}_daily.csv\")\n",
    "    print(f\"{symbol} saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [20,320]\n",
    "\n",
    "def zscore(x, window):\n",
    "    r = x.rolling(window=window)\n",
    "    m = r.mean().shift(1)\n",
    "    s = r.std(ddof=0).shift(1)\n",
    "    z = (x-m)/s\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in listdir('../data/predict/') if isfile(join('../data/predict/', f))]\n",
    "onlyfiles = list(filter(lambda thisfilename: '_daily.csv' in thisfilename, onlyfiles))\n",
    "\n",
    "ticker_list = []\n",
    "ticker_stats_mean = pd.DataFrame()\n",
    "ticker_stats_std = pd.DataFrame()\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    for filename in onlyfiles:\n",
    "        ticker = filename.split('_')[0]\n",
    "        ticker_list.append(ticker)\n",
    "        \n",
    "        # import csv, sort by date so percent change works, drop unneeded columns,\n",
    "        # rename columns, calculate moving averages, calulate percent changes, drop na's\n",
    "        df = pd.read_csv(f\"../data/predict/{ticker}_daily.csv\")\n",
    "        df.sort_index(inplace=True, ascending=False)\n",
    "        df = df.drop(['5. adjusted close', '7. dividend amount', '8. split coefficient','SMA','EMA'], axis=1)\n",
    "        df.columns = ['date','open','high','low','close','volume','rsi']\n",
    "\n",
    "        # moving averages, convert to percentage of close\n",
    "        df['sma'] = df.iloc[:,3].rolling(window=50).mean()/df.iloc[:,3]\n",
    "        df['ema'] = df.iloc[:,3].ewm(span=21).mean()/df.iloc[:,3]\n",
    "\n",
    "        # percent change\n",
    "        df['open'] = df['open'].pct_change()\n",
    "        df['high'] = df['high'].pct_change()\n",
    "        df['low'] = df['low'].pct_change()\n",
    "        df['close'] = df['close'].pct_change()\n",
    "        df['volume'] = df['volume'].pct_change()\n",
    "        df = df.dropna()\n",
    "\n",
    "        # zscore\n",
    "        df['open'] =zscore(df['open'], window=window_size)\n",
    "        df['high'] = zscore(df['high'], window=window_size)\n",
    "        df['low'] = zscore(df['low'], window=window_size)\n",
    "        df['close'] = zscore(df['close'], window=window_size)\n",
    "        df['volume'] = zscore(df['volume'], window=window_size)\n",
    "        df['rsi'] = zscore(df['rsi'], window=window_size)\n",
    "        df['sma'] = zscore(df['sma'], window=window_size)\n",
    "        df['ema'] = zscore(df['ema'], window=window_size)\n",
    "        df = df.dropna()\n",
    "\n",
    "        # write data and describe to csv\n",
    "        df.to_csv(f\"../data/predict/{ticker}_{window_size}_processed.csv\")\n",
    "\n",
    "ticker_list = set(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting window size: 20\n",
      "starting window size: 320\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {}\n",
    "\n",
    "# iterating through the window sizes provided in the list above\n",
    "for window_size in window_sizes:\n",
    "    print(f\"starting window size: {window_size}\")\n",
    "    \n",
    "    # iterating through the list of processed files\n",
    "    for ticker in ticker_list:\n",
    "        # empty dataframe for this ticker's flattened data\n",
    "        flattened_df = pd.DataFrame()\n",
    "        df = pd.read_csv(f\"../data/predict/{ticker}_{window_size}_processed.csv\").drop(['Unnamed: 0'], axis=1)\n",
    "        \n",
    "        # iterating through grouped rows (window size)\n",
    "        for i in range(df.shape[0]-window_size-1, df.shape[0]-window_size+1):\n",
    "            # resetting the index each time so column names align\n",
    "            df_window = df.iloc[i:i+window_size,].reset_index(drop=True)\n",
    "            # stats dict\n",
    "            if i == (df.shape[0]-window_size):\n",
    "                stats_dict[ticker] = {'open': {'mean': df_window['open'].mean(), 'std': df_window['open'].std()},\n",
    "                                      'high': {'mean': df_window['high'].mean(), 'std': df_window['high'].std()},\n",
    "                                      'low': {'mean': df_window['low'].mean(), 'std': df_window['low'].std()},\n",
    "                                      'close': {'mean': df_window['close'].mean(), 'std': df_window['close'].std()}}\n",
    "            # mapping the index as a string so it can be concatenated\n",
    "            df_window.index = df_window.index.map(str)\n",
    "            # unstacking the window to one row\n",
    "            df_window = df_window.unstack().to_frame().sort_index(level=1).T\n",
    "            # renaming the columns\n",
    "            df_window.columns = df_window.columns.map('_'.join)\n",
    "            # concatenating the flattened row to the dataframe\n",
    "            flattened_df = pd.concat([flattened_df, df_window], axis=0)\n",
    "            \n",
    "                \n",
    "        # writing the flattened dataframe for this ticker and window size to csv\n",
    "        flattened_df.to_csv(f\"../data/predict/{ticker}_{window_size}_flattened.csv\")\n",
    "        #print(f\"{ticker} {window_size} flattened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (c:\\Users\\trent\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:301742)",
      "at S.execute (c:\\Users\\trent\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:300732)",
      "at S.start (c:\\Users\\trent\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:296408)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (c:\\Users\\trent\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:312326)",
      "at t.CellExecutionQueue.start (c:\\Users\\trent\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "forecast_dict = {}\n",
    "\n",
    "for ticker in ticker_list:\n",
    "\n",
    "    forecast_dict[ticker] = {}\n",
    "\n",
    "    for window_size in [20, 320]:\n",
    "\n",
    "        onlyfiles = [f for f in listdir('./results') if isfile(join('./results', f))]\n",
    "        onlyfiles = list(filter(lambda thisfilename: f\"-{window_size}-\" in thisfilename, onlyfiles))\n",
    "\n",
    "        # import csv, sort by date so percent change works, drop unneeded columns,\n",
    "        # rename columns, calculate moving averages, calulate percent changes, drop na's\n",
    "        df = pd.read_csv(f\"../data/predict/{ticker}_{window_size}_flattened.csv\")\n",
    "        df_daily = pd.read_csv(f\"../data/predict/{ticker}_daily.csv\")\n",
    "        df_daily = df_daily.drop(['5. adjusted close', '6. volume', '7. dividend amount', '8. split coefficient','SMA','EMA','RSI'], axis=1)\n",
    "        df_daily.columns = ['date','open','high','low','close']\n",
    "        print(f\"Latest date that data is available for in ../data/predict/{ticker}_{window_size}_flattened.csv is {df_daily.date[0]} (should be latest trading day).\")\n",
    "\n",
    "        # features to use\n",
    "        items = [\"close\", \"ema\", \"high\", \"low\", \"open\", \"rsi\", \"sma\", \"volume\"]\n",
    "        day_counts = [f\"_{i}\" for i in range(1, window_size)]\n",
    "        FEATURE_COLUMNS = []\n",
    "        for day_count in day_counts:\n",
    "            for item in items:\n",
    "                FEATURE_COLUMNS.append(f\"{item}{day_count}\")\n",
    "\n",
    "        df_pred = df[FEATURE_COLUMNS]\n",
    "\n",
    "        # Get samples to predict\n",
    "        samples_to_predict = np.array(df_pred)\n",
    "\n",
    "        # Convert into Numpy array\n",
    "        samples_to_predict = samples_to_predict.reshape((samples_to_predict.shape[0], 1, samples_to_predict.shape[1]))\n",
    "\n",
    "        # File path\n",
    "        for model_filepath in onlyfiles:\n",
    "            print(f\"Starting {model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]} for {ticker}.\")\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]] = {'window_size': window_size, 'tomorrow_forecast_open': None, 'tomorrow_forecast_high': None, 'tomorrow_forecast_low': None, 'tomorrow_forecast_close': None,\n",
    "                                    'today_forecast_open': None, 'today_forecast_high': None, 'today_forecast_low': None, 'today_forecast_close': None,\n",
    "                                    'today_actual_open': None, 'today_actual_high': None, 'today_actual_low': None, 'today_actual_close': None, \n",
    "                                    'today_error_open': None, 'today_error_high': None, 'today_error_low': None, 'today_error_close': None}\n",
    "            model = load_model(f\"./results/{model_filepath}\", compile=True)\n",
    "            # Predict\n",
    "            predictions = model.predict(samples_to_predict)\n",
    "            \n",
    "            linear_df = pd.DataFrame(predictions)\n",
    "            linear_df.columns = ['close','high','low','open']\n",
    "            linear_df.index = ['today','tomorrow']\n",
    "\n",
    "            linear_df.close = linear_df.close*stats_dict[ticker]['close']['std'] + stats_dict[ticker]['close']['mean']\n",
    "            linear_df.high = linear_df.high*stats_dict[ticker]['high']['std'] + stats_dict[ticker]['high']['mean']\n",
    "            linear_df.low = linear_df.low*stats_dict[ticker]['low']['std'] + stats_dict[ticker]['low']['mean']\n",
    "            linear_df.open = linear_df.open*stats_dict[ticker]['open']['std'] + stats_dict[ticker]['open']['mean']\n",
    "\n",
    "            linear_df = linear_df[['open','high','low','close']]\n",
    "            \n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_open'] = df_daily[['open']].iloc[0,][0] + (df_daily[['open']].iloc[0,][0] * (linear_df[['open']].iloc[1,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_high'] = df_daily[['high']].iloc[0,][0] + (df_daily[['high']].iloc[0,][0] * (linear_df[['high']].iloc[1,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_low'] = df_daily[['low']].iloc[0,][0] + (df_daily[['low']].iloc[0,][0] * (linear_df[['low']].iloc[1,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_close'] = df_daily[['close']].iloc[0,][0] + (df_daily[['close']].iloc[0,][0] * (linear_df[['close']].iloc[1,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_open_pct'] = ((df_daily[['open']].iloc[0,][0] + (df_daily[['open']].iloc[0,][0] * (linear_df[['open']].iloc[1,][0]/100))) - df_daily[['close']].iloc[0,][0])/df_daily[['close']].iloc[0,][0] * 100\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_high_pct'] = ((df_daily[['high']].iloc[0,][0] + (df_daily[['high']].iloc[0,][0] * (linear_df[['high']].iloc[1,][0]/100))) - df_daily[['close']].iloc[0,][0])/df_daily[['close']].iloc[0,][0] * 100\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_low_pct'] = ((df_daily[['low']].iloc[0,][0] + (df_daily[['low']].iloc[0,][0] * (linear_df[['low']].iloc[1,][0]/100))) - df_daily[['close']].iloc[0,][0])/df_daily[['close']].iloc[0,][0] * 100\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['tomorrow_forecast_close_pct'] = ((df_daily[['close']].iloc[0,][0] + (df_daily[['close']].iloc[0,][0] * (linear_df[['close']].iloc[1,][0]/100))) - df_daily[['close']].iloc[0,][0])/df_daily[['close']].iloc[0,][0] * 100\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_open'] = df_daily[['open']].iloc[1,][0] + (df_daily[['open']].iloc[1,][0] * (linear_df[['open']].iloc[0,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_high'] = df_daily[['high']].iloc[1,][0] + (df_daily[['high']].iloc[1,][0] * (linear_df[['high']].iloc[0,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_low'] = df_daily[['low']].iloc[1,][0] + (df_daily[['low']].iloc[1,][0] * (linear_df[['low']].iloc[0,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_close'] = df_daily[['close']].iloc[1,][0] + (df_daily[['close']].iloc[1,][0] * (linear_df[['close']].iloc[0,][0]/100))\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_open'] = df_daily[['open']].iloc[0,][0]\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_high'] = df_daily[['high']].iloc[0,][0]\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_low'] = df_daily[['low']].iloc[0,][0]\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_close'] = df_daily[['close']].iloc[0,][0]\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_error_open'] = abs((forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_open'] - forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_open']) / forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_open'] * 100)\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_error_high'] = abs((forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_high'] - forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_high']) / forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_high'] * 100)\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_error_low'] = abs((forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_low'] - forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_low']) / forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_low'] * 100)\n",
    "            forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_error_close'] = abs((forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_forecast_close'] - forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_close']) / forecast_dict[ticker][model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]]['today_actual_close'] * 100)\n",
    "            \n",
    "            print(f\"Finished {model_filepath.split('-')[7] + '-' + model_filepath.split('-')[3]} for {ticker}.\")\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    pd.DataFrame(forecast_dict[ticker]).to_csv(f\"../data/predict_tomorrow/{ticker}_{df_daily.date[0]}_daily_forecast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4c1afcaa0824698e49fb009c9da9eaf010fa52d2eadd2a196450101a9336fb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('beta_lactamase': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
