{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import multiprocessing\n",
    "    \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    \n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False,layer_activation=\"linear\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=layer_activation))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67485 samples, validate on 28923 samples\n",
      "Epoch 1/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.5230 - mean_absolute_error: 0.8953\n",
      "Epoch 00001: val_loss improved from inf to 0.52236, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 24s 355us/sample - loss: 0.5229 - mean_absolute_error: 0.8952 - val_loss: 0.5224 - val_mean_absolute_error: 0.8931\n",
      "Epoch 2/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5124 - mean_absolute_error: 0.8826\n",
      "Epoch 00002: val_loss improved from 0.52236 to 0.51354, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 241us/sample - loss: 0.5127 - mean_absolute_error: 0.8829 - val_loss: 0.5135 - val_mean_absolute_error: 0.8831\n",
      "Epoch 3/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.5078 - mean_absolute_error: 0.8772\n",
      "Epoch 00003: val_loss improved from 0.51354 to 0.51063, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.5078 - mean_absolute_error: 0.8771 - val_loss: 0.5106 - val_mean_absolute_error: 0.8794\n",
      "Epoch 4/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5027 - mean_absolute_error: 0.8708\n",
      "Epoch 00004: val_loss improved from 0.51063 to 0.50987, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.5027 - mean_absolute_error: 0.8708 - val_loss: 0.5099 - val_mean_absolute_error: 0.8789\n",
      "Epoch 5/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4969 - mean_absolute_error: 0.8644\n",
      "Epoch 00005: val_loss improved from 0.50987 to 0.50458, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.4968 - mean_absolute_error: 0.8643 - val_loss: 0.5046 - val_mean_absolute_error: 0.8727\n",
      "Epoch 6/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4884 - mean_absolute_error: 0.8544\n",
      "Epoch 00006: val_loss improved from 0.50458 to 0.50286, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 248us/sample - loss: 0.4902 - mean_absolute_error: 0.8562 - val_loss: 0.5029 - val_mean_absolute_error: 0.8700\n",
      "Epoch 7/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4834 - mean_absolute_error: 0.8479\n",
      "Epoch 00007: val_loss improved from 0.50286 to 0.49916, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 254us/sample - loss: 0.4834 - mean_absolute_error: 0.8479 - val_loss: 0.4992 - val_mean_absolute_error: 0.8662\n",
      "Epoch 8/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4753 - mean_absolute_error: 0.8381\n",
      "Epoch 00008: val_loss improved from 0.49916 to 0.49726, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4753 - mean_absolute_error: 0.8382 - val_loss: 0.4973 - val_mean_absolute_error: 0.8641\n",
      "Epoch 9/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4676 - mean_absolute_error: 0.8291\n",
      "Epoch 00009: val_loss improved from 0.49726 to 0.49558, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.4676 - mean_absolute_error: 0.8291 - val_loss: 0.4956 - val_mean_absolute_error: 0.8614\n",
      "Epoch 10/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4596 - mean_absolute_error: 0.8197\n",
      "Epoch 00010: val_loss improved from 0.49558 to 0.49401, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 252us/sample - loss: 0.4596 - mean_absolute_error: 0.8197 - val_loss: 0.4940 - val_mean_absolute_error: 0.8599\n",
      "Epoch 11/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4513 - mean_absolute_error: 0.8102\n",
      "Epoch 00011: val_loss improved from 0.49401 to 0.49353, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.4513 - mean_absolute_error: 0.8102 - val_loss: 0.4935 - val_mean_absolute_error: 0.8590\n",
      "Epoch 12/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.4443 - mean_absolute_error: 0.8017\n",
      "Epoch 00012: val_loss improved from 0.49353 to 0.49350, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4442 - mean_absolute_error: 0.8016 - val_loss: 0.4935 - val_mean_absolute_error: 0.8595\n",
      "Epoch 13/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.4362 - mean_absolute_error: 0.7919\n",
      "Epoch 00013: val_loss improved from 0.49350 to 0.48951, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.4362 - mean_absolute_error: 0.7918 - val_loss: 0.4895 - val_mean_absolute_error: 0.8544\n",
      "Epoch 14/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4279 - mean_absolute_error: 0.7824\n",
      "Epoch 00014: val_loss improved from 0.48951 to 0.48775, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.4279 - mean_absolute_error: 0.7824 - val_loss: 0.4877 - val_mean_absolute_error: 0.8524\n",
      "Epoch 15/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4198 - mean_absolute_error: 0.7722\n",
      "Epoch 00015: val_loss improved from 0.48775 to 0.48619, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.4199 - mean_absolute_error: 0.7723 - val_loss: 0.4862 - val_mean_absolute_error: 0.8507\n",
      "Epoch 16/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4129 - mean_absolute_error: 0.7643\n",
      "Epoch 00016: val_loss improved from 0.48619 to 0.48541, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4129 - mean_absolute_error: 0.7643 - val_loss: 0.4854 - val_mean_absolute_error: 0.8495\n",
      "Epoch 17/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4067 - mean_absolute_error: 0.7569\n",
      "Epoch 00017: val_loss improved from 0.48541 to 0.48487, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4067 - mean_absolute_error: 0.7569 - val_loss: 0.4849 - val_mean_absolute_error: 0.8496\n",
      "Epoch 18/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4006 - mean_absolute_error: 0.7495\n",
      "Epoch 00018: val_loss improved from 0.48487 to 0.48326, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.4007 - mean_absolute_error: 0.7496 - val_loss: 0.4833 - val_mean_absolute_error: 0.8475\n",
      "Epoch 19/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3936 - mean_absolute_error: 0.7408\n",
      "Epoch 00019: val_loss did not improve from 0.48326\n",
      "67485/67485 [==============================] - 17s 249us/sample - loss: 0.3936 - mean_absolute_error: 0.7407 - val_loss: 0.4850 - val_mean_absolute_error: 0.8493\n",
      "Epoch 20/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3881 - mean_absolute_error: 0.7351\n",
      "Epoch 00020: val_loss did not improve from 0.48326\n",
      "67485/67485 [==============================] - 16s 243us/sample - loss: 0.3881 - mean_absolute_error: 0.7351 - val_loss: 0.4835 - val_mean_absolute_error: 0.8476\n",
      "Epoch 21/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3829 - mean_absolute_error: 0.7283\n",
      "Epoch 00021: val_loss improved from 0.48326 to 0.48136, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.3829 - mean_absolute_error: 0.7283 - val_loss: 0.4814 - val_mean_absolute_error: 0.8447\n",
      "Epoch 22/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3784 - mean_absolute_error: 0.7233\n",
      "Epoch 00022: val_loss improved from 0.48136 to 0.48055, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.3784 - mean_absolute_error: 0.7233 - val_loss: 0.4806 - val_mean_absolute_error: 0.8438\n",
      "Epoch 23/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3742 - mean_absolute_error: 0.7180\n",
      "Epoch 00023: val_loss improved from 0.48055 to 0.47969, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.3743 - mean_absolute_error: 0.7181 - val_loss: 0.4797 - val_mean_absolute_error: 0.8432\n",
      "Epoch 24/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3695 - mean_absolute_error: 0.7118\n",
      "Epoch 00024: val_loss improved from 0.47969 to 0.47897, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 241us/sample - loss: 0.3695 - mean_absolute_error: 0.7117 - val_loss: 0.4790 - val_mean_absolute_error: 0.8421\n",
      "Epoch 25/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3661 - mean_absolute_error: 0.7084\n",
      "Epoch 00025: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 16s 240us/sample - loss: 0.3661 - mean_absolute_error: 0.7084 - val_loss: 0.4826 - val_mean_absolute_error: 0.8466\n",
      "Epoch 26/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3616 - mean_absolute_error: 0.7024\n",
      "Epoch 00026: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.3616 - mean_absolute_error: 0.7024 - val_loss: 0.4791 - val_mean_absolute_error: 0.8424\n",
      "Epoch 27/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3576 - mean_absolute_error: 0.6982\n",
      "Epoch 00027: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.3576 - mean_absolute_error: 0.6982 - val_loss: 0.4792 - val_mean_absolute_error: 0.8421\n",
      "Epoch 28/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.3542 - mean_absolute_error: 0.6935\n",
      "Epoch 00028: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.3542 - mean_absolute_error: 0.6934 - val_loss: 0.4795 - val_mean_absolute_error: 0.8426\n",
      "Epoch 29/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3515 - mean_absolute_error: 0.6900\n",
      "Epoch 00029: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 17s 248us/sample - loss: 0.3515 - mean_absolute_error: 0.6900 - val_loss: 0.4817 - val_mean_absolute_error: 0.8454\n",
      "Epoch 30/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3479 - mean_absolute_error: 0.6860\n",
      "Epoch 00030: val_loss did not improve from 0.47897\n",
      "67485/67485 [==============================] - 17s 255us/sample - loss: 0.3479 - mean_absolute_error: 0.6860 - val_loss: 0.4854 - val_mean_absolute_error: 0.8498\n",
      "Epoch 31/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.3453 - mean_absolute_error: 0.6826\n",
      "Epoch 00031: val_loss improved from 0.47897 to 0.47634, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 17s 249us/sample - loss: 0.3453 - mean_absolute_error: 0.6826 - val_loss: 0.4763 - val_mean_absolute_error: 0.8386\n",
      "Epoch 32/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.3436 - mean_absolute_error: 0.6809\n",
      "Epoch 00032: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 17s 251us/sample - loss: 0.3437 - mean_absolute_error: 0.6811 - val_loss: 0.4815 - val_mean_absolute_error: 0.8448\n",
      "Epoch 33/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3412 - mean_absolute_error: 0.6777\n",
      "Epoch 00033: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.3412 - mean_absolute_error: 0.6777 - val_loss: 0.4773 - val_mean_absolute_error: 0.8399\n",
      "Epoch 34/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3382 - mean_absolute_error: 0.6745\n",
      "Epoch 00034: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 16s 238us/sample - loss: 0.3382 - mean_absolute_error: 0.6745 - val_loss: 0.4782 - val_mean_absolute_error: 0.8412\n",
      "Epoch 35/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3365 - mean_absolute_error: 0.6719\n",
      "Epoch 00035: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.3365 - mean_absolute_error: 0.6718 - val_loss: 0.4782 - val_mean_absolute_error: 0.8410\n",
      "Epoch 36/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3344 - mean_absolute_error: 0.6692\n",
      "Epoch 00036: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 16s 239us/sample - loss: 0.3345 - mean_absolute_error: 0.6694 - val_loss: 0.4792 - val_mean_absolute_error: 0.8425\n",
      "Epoch 37/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3320 - mean_absolute_error: 0.6666\n",
      "Epoch 00037: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 17s 245us/sample - loss: 0.3321 - mean_absolute_error: 0.6666 - val_loss: 0.4804 - val_mean_absolute_error: 0.8438\n",
      "Epoch 38/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.3296 - mean_absolute_error: 0.6639\n",
      "Epoch 00038: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.3296 - mean_absolute_error: 0.6639 - val_loss: 0.4769 - val_mean_absolute_error: 0.8393\n",
      "Epoch 39/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3271 - mean_absolute_error: 0.6605\n",
      "Epoch 00039: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 17s 246us/sample - loss: 0.3271 - mean_absolute_error: 0.6605 - val_loss: 0.4801 - val_mean_absolute_error: 0.8430\n",
      "Epoch 40/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.3253 - mean_absolute_error: 0.6584\n",
      "Epoch 00040: val_loss did not improve from 0.47634\n",
      "67485/67485 [==============================] - 16s 242us/sample - loss: 0.3253 - mean_absolute_error: 0.6584 - val_loss: 0.4813 - val_mean_absolute_error: 0.8443\n",
      "Epoch 41/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3246 - mean_absolute_error: 0.6571\n",
      "Epoch 00041: val_loss improved from 0.47634 to 0.47529, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 16s 244us/sample - loss: 0.3245 - mean_absolute_error: 0.6570 - val_loss: 0.4753 - val_mean_absolute_error: 0.8372\n",
      "Epoch 42/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3227 - mean_absolute_error: 0.6549\n",
      "Epoch 00042: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 16s 241us/sample - loss: 0.3227 - mean_absolute_error: 0.6549 - val_loss: 0.4816 - val_mean_absolute_error: 0.8451\n",
      "Epoch 43/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3204 - mean_absolute_error: 0.6521\n",
      "Epoch 00043: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 16s 241us/sample - loss: 0.3204 - mean_absolute_error: 0.6521 - val_loss: 0.4792 - val_mean_absolute_error: 0.8420\n",
      "Epoch 44/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.3206 - mean_absolute_error: 0.6523\n",
      "Epoch 00044: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 16s 237us/sample - loss: 0.3207 - mean_absolute_error: 0.6525 - val_loss: 0.4770 - val_mean_absolute_error: 0.8389\n",
      "Epoch 45/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3183 - mean_absolute_error: 0.6496\n",
      "Epoch 00045: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 17s 251us/sample - loss: 0.3182 - mean_absolute_error: 0.6496 - val_loss: 0.4754 - val_mean_absolute_error: 0.8372\n",
      "Epoch 46/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.3176 - mean_absolute_error: 0.6489\n",
      "Epoch 00046: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 17s 250us/sample - loss: 0.3176 - mean_absolute_error: 0.6489 - val_loss: 0.4769 - val_mean_absolute_error: 0.8395\n",
      "Epoch 47/50\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3164 - mean_absolute_error: 0.6470\n",
      "Epoch 00047: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 17s 256us/sample - loss: 0.3164 - mean_absolute_error: 0.6470 - val_loss: 0.4786 - val_mean_absolute_error: 0.8408\n",
      "Epoch 48/50\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3151 - mean_absolute_error: 0.6460\n",
      "Epoch 00048: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 18s 262us/sample - loss: 0.3151 - mean_absolute_error: 0.6460 - val_loss: 0.4765 - val_mean_absolute_error: 0.8388\n",
      "Epoch 49/50\n",
      "67328/67485 [============================>.] - ETA: 0s - loss: 0.3121 - mean_absolute_error: 0.6416\n",
      "Epoch 00049: val_loss did not improve from 0.47529\n",
      "67485/67485 [==============================] - 18s 272us/sample - loss: 0.3122 - mean_absolute_error: 0.6417 - val_loss: 0.4773 - val_mean_absolute_error: 0.8390\n",
      "Epoch 50/50\n",
      "67264/67485 [============================>.] - ETA: 0s - loss: 0.3121 - mean_absolute_error: 0.6419\n",
      "Epoch 00050: val_loss improved from 0.47529 to 0.47511, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-256-b.h5\n",
      "67485/67485 [==============================] - 19s 284us/sample - loss: 0.3122 - mean_absolute_error: 0.6421 - val_loss: 0.4751 - val_mean_absolute_error: 0.8365\n"
     ]
    }
   ],
   "source": [
    "#def run_tensorflow():\n",
    "\n",
    "# create these folders if they does not exist\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 72\n",
    "# Lookup step, 1 is the next day\n",
    "#LOOKUP_STEP = int(run_dict[run][\"LOOKUP_STEP\"])\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.3\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"close_0\",\"ema_0\",\"high_0\",\"low_0\",\"open_0\",\"rsi_0\",\"sma_0\",\"volume_0\",\"close_1\",\"ema_1\",\"high_1\",\"low_1\",\"open_1\",\"rsi_1\",\"sma_1\",\"volume_1\",\n",
    "                   \"close_2\",\"ema_2\",\"high_2\",\"low_2\",\"open_2\",\"rsi_2\",\"sma_2\",\"volume_2\",\"close_3\",\"ema_3\",\"high_3\",\"low_3\",\"open_3\",\"rsi_3\",\"sma_3\",\"volume_3\",\n",
    "                   \"close_4\",\"ema_4\",\"high_4\",\"low_4\",\"open_4\",\"rsi_4\",\"sma_4\",\"volume_4\",\"close_5\",\"ema_5\",\"high_5\",\"low_5\",\"open_5\",\"rsi_5\",\"sma_5\",\"volume_5\",\n",
    "                   \"close_6\",\"ema_6\",\"high_6\",\"low_6\",\"open_6\",\"rsi_6\",\"sma_6\",\"volume_6\",\"close_7\",\"ema_7\",\"high_7\",\"low_7\",\"open_7\",\"rsi_7\",\"sma_7\",\"volume_7\",\n",
    "                   \"close_8\",\"ema_8\",\"high_8\",\"low_8\",\"open_8\",\"rsi_8\",\"sma_8\",\"volume_8\"]\n",
    "TARGET_COLUMNS = [\"close_9\",\"high_9\",\"low_9\",\"open_9\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.3\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "LAYER_ACTIVATION = \"linear\"\n",
    "\n",
    "# Stock market\n",
    "ticker = \"MIXED\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-{LAYER_ACTIVATION}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#try:\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/processed/all_processed_10.csv')\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL, layer_activation=LAYER_ACTIVATION)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "X = data[FEATURE_COLUMNS]\n",
    "y = data[TARGET_COLUMNS]\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# reshape X to fit the neural network\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")\n",
    "\n",
    "#except:\n",
    "#    print(\"There was an attempt.\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
