{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import multiprocessing\n",
    "    \n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    \n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False,layer_activation=\"linear\"):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=layer_activation))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67485 samples, validate on 28923 samples\n",
      "Epoch 1/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.5290 - mean_absolute_error: 0.9017\n",
      "Epoch 00001: val_loss improved from inf to 0.51433, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 43s 641us/sample - loss: 0.5290 - mean_absolute_error: 0.9017 - val_loss: 0.5143 - val_mean_absolute_error: 0.8855\n",
      "Epoch 2/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5192 - mean_absolute_error: 0.8900\n",
      "Epoch 00002: val_loss improved from 0.51433 to 0.50869, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 35s 513us/sample - loss: 0.5192 - mean_absolute_error: 0.8900 - val_loss: 0.5087 - val_mean_absolute_error: 0.8789\n",
      "Epoch 3/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5129 - mean_absolute_error: 0.8826\n",
      "Epoch 00003: val_loss improved from 0.50869 to 0.50849, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 35s 521us/sample - loss: 0.5127 - mean_absolute_error: 0.8825 - val_loss: 0.5085 - val_mean_absolute_error: 0.8791\n",
      "Epoch 4/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.5058 - mean_absolute_error: 0.8739\n",
      "Epoch 00004: val_loss improved from 0.50849 to 0.50397, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 36s 527us/sample - loss: 0.5057 - mean_absolute_error: 0.8737 - val_loss: 0.5040 - val_mean_absolute_error: 0.8727\n",
      "Epoch 5/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4964 - mean_absolute_error: 0.8629\n",
      "Epoch 00005: val_loss improved from 0.50397 to 0.50087, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 34s 498us/sample - loss: 0.4964 - mean_absolute_error: 0.8628 - val_loss: 0.5009 - val_mean_absolute_error: 0.8692\n",
      "Epoch 6/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4849 - mean_absolute_error: 0.8492\n",
      "Epoch 00006: val_loss improved from 0.50087 to 0.49192, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 480us/sample - loss: 0.4850 - mean_absolute_error: 0.8493 - val_loss: 0.4919 - val_mean_absolute_error: 0.8592\n",
      "Epoch 7/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4686 - mean_absolute_error: 0.8300\n",
      "Epoch 00007: val_loss improved from 0.49192 to 0.48769, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 480us/sample - loss: 0.4685 - mean_absolute_error: 0.8299 - val_loss: 0.4877 - val_mean_absolute_error: 0.8544\n",
      "Epoch 8/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4495 - mean_absolute_error: 0.8069\n",
      "Epoch 00008: val_loss improved from 0.48769 to 0.48280, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 480us/sample - loss: 0.4496 - mean_absolute_error: 0.8070 - val_loss: 0.4828 - val_mean_absolute_error: 0.8482\n",
      "Epoch 9/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.4283 - mean_absolute_error: 0.7816\n",
      "Epoch 00009: val_loss improved from 0.48280 to 0.47827, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 480us/sample - loss: 0.4282 - mean_absolute_error: 0.7815 - val_loss: 0.4783 - val_mean_absolute_error: 0.8426\n",
      "Epoch 10/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.4058 - mean_absolute_error: 0.7542\n",
      "Epoch 00010: val_loss did not improve from 0.47827\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.4059 - mean_absolute_error: 0.7543 - val_loss: 0.4824 - val_mean_absolute_error: 0.8471\n",
      "Epoch 11/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3853 - mean_absolute_error: 0.7296\n",
      "Epoch 00011: val_loss improved from 0.47827 to 0.47593, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 33s 490us/sample - loss: 0.3852 - mean_absolute_error: 0.7295 - val_loss: 0.4759 - val_mean_absolute_error: 0.8396\n",
      "Epoch 12/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3654 - mean_absolute_error: 0.7061\n",
      "Epoch 00012: val_loss improved from 0.47593 to 0.47371, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 33s 483us/sample - loss: 0.3654 - mean_absolute_error: 0.7061 - val_loss: 0.4737 - val_mean_absolute_error: 0.8365\n",
      "Epoch 13/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3479 - mean_absolute_error: 0.6844\n",
      "Epoch 00013: val_loss did not improve from 0.47371\n",
      "67485/67485 [==============================] - 33s 486us/sample - loss: 0.3479 - mean_absolute_error: 0.6844 - val_loss: 0.4798 - val_mean_absolute_error: 0.8441\n",
      "Epoch 14/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3334 - mean_absolute_error: 0.6666\n",
      "Epoch 00014: val_loss improved from 0.47371 to 0.47267, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 33s 488us/sample - loss: 0.3334 - mean_absolute_error: 0.6666 - val_loss: 0.4727 - val_mean_absolute_error: 0.8360\n",
      "Epoch 15/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.3210 - mean_absolute_error: 0.6510\n",
      "Epoch 00015: val_loss improved from 0.47267 to 0.46970, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 33s 484us/sample - loss: 0.3210 - mean_absolute_error: 0.6511 - val_loss: 0.4697 - val_mean_absolute_error: 0.8321\n",
      "Epoch 16/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3099 - mean_absolute_error: 0.6374\n",
      "Epoch 00016: val_loss improved from 0.46970 to 0.46634, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 479us/sample - loss: 0.3100 - mean_absolute_error: 0.6375 - val_loss: 0.4663 - val_mean_absolute_error: 0.8279\n",
      "Epoch 17/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.3008 - mean_absolute_error: 0.6257\n",
      "Epoch 00017: val_loss did not improve from 0.46634\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.3008 - mean_absolute_error: 0.6257 - val_loss: 0.4677 - val_mean_absolute_error: 0.8296\n",
      "Epoch 18/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2934 - mean_absolute_error: 0.6168\n",
      "Epoch 00018: val_loss did not improve from 0.46634\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2934 - mean_absolute_error: 0.6168 - val_loss: 0.4681 - val_mean_absolute_error: 0.8297\n",
      "Epoch 19/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2871 - mean_absolute_error: 0.6088\n",
      "Epoch 00019: val_loss did not improve from 0.46634\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2872 - mean_absolute_error: 0.6088 - val_loss: 0.4677 - val_mean_absolute_error: 0.8292\n",
      "Epoch 20/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2809 - mean_absolute_error: 0.6003\n",
      "Epoch 00020: val_loss improved from 0.46634 to 0.46502, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 478us/sample - loss: 0.2810 - mean_absolute_error: 0.6003 - val_loss: 0.4650 - val_mean_absolute_error: 0.8265\n",
      "Epoch 21/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2771 - mean_absolute_error: 0.5952\n",
      "Epoch 00021: val_loss improved from 0.46502 to 0.46425, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 478us/sample - loss: 0.2771 - mean_absolute_error: 0.5953 - val_loss: 0.4642 - val_mean_absolute_error: 0.8249\n",
      "Epoch 22/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2721 - mean_absolute_error: 0.5888\n",
      "Epoch 00022: val_loss improved from 0.46425 to 0.46007, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2721 - mean_absolute_error: 0.5889 - val_loss: 0.4601 - val_mean_absolute_error: 0.8200\n",
      "Epoch 23/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2684 - mean_absolute_error: 0.5840\n",
      "Epoch 00023: val_loss did not improve from 0.46007\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2684 - mean_absolute_error: 0.5840 - val_loss: 0.4615 - val_mean_absolute_error: 0.8216\n",
      "Epoch 24/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2655 - mean_absolute_error: 0.5802\n",
      "Epoch 00024: val_loss did not improve from 0.46007\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2655 - mean_absolute_error: 0.5802 - val_loss: 0.4619 - val_mean_absolute_error: 0.8219\n",
      "Epoch 25/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2626 - mean_absolute_error: 0.5765\n",
      "Epoch 00025: val_loss did not improve from 0.46007\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2626 - mean_absolute_error: 0.5765 - val_loss: 0.4645 - val_mean_absolute_error: 0.8254\n",
      "Epoch 26/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2595 - mean_absolute_error: 0.5723\n",
      "Epoch 00026: val_loss improved from 0.46007 to 0.45904, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2595 - mean_absolute_error: 0.5722 - val_loss: 0.4590 - val_mean_absolute_error: 0.8181\n",
      "Epoch 27/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2551 - mean_absolute_error: 0.5667\n",
      "Epoch 00027: val_loss did not improve from 0.45904\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2551 - mean_absolute_error: 0.5667 - val_loss: 0.4595 - val_mean_absolute_error: 0.8184\n",
      "Epoch 28/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2542 - mean_absolute_error: 0.5657\n",
      "Epoch 00028: val_loss improved from 0.45904 to 0.45857, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2541 - mean_absolute_error: 0.5657 - val_loss: 0.4586 - val_mean_absolute_error: 0.8170\n",
      "Epoch 29/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2514 - mean_absolute_error: 0.5621\n",
      "Epoch 00029: val_loss improved from 0.45857 to 0.45786, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2514 - mean_absolute_error: 0.5621 - val_loss: 0.4579 - val_mean_absolute_error: 0.8168\n",
      "Epoch 30/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2498 - mean_absolute_error: 0.5601\n",
      "Epoch 00030: val_loss improved from 0.45786 to 0.45564, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2498 - mean_absolute_error: 0.5601 - val_loss: 0.4556 - val_mean_absolute_error: 0.8140\n",
      "Epoch 31/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2485 - mean_absolute_error: 0.5577\n",
      "Epoch 00031: val_loss did not improve from 0.45564\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2485 - mean_absolute_error: 0.5576 - val_loss: 0.4572 - val_mean_absolute_error: 0.8155\n",
      "Epoch 32/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2470 - mean_absolute_error: 0.5561\n",
      "Epoch 00032: val_loss improved from 0.45564 to 0.45530, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2470 - mean_absolute_error: 0.5561 - val_loss: 0.4553 - val_mean_absolute_error: 0.8137\n",
      "Epoch 33/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2450 - mean_absolute_error: 0.5535\n",
      "Epoch 00033: val_loss did not improve from 0.45530\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2450 - mean_absolute_error: 0.5535 - val_loss: 0.4577 - val_mean_absolute_error: 0.8163\n",
      "Epoch 34/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2439 - mean_absolute_error: 0.5517\n",
      "Epoch 00034: val_loss did not improve from 0.45530\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2439 - mean_absolute_error: 0.5516 - val_loss: 0.4561 - val_mean_absolute_error: 0.8147\n",
      "Epoch 35/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2425 - mean_absolute_error: 0.5501\n",
      "Epoch 00035: val_loss improved from 0.45530 to 0.45456, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2425 - mean_absolute_error: 0.5501 - val_loss: 0.4546 - val_mean_absolute_error: 0.8127\n",
      "Epoch 36/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2430 - mean_absolute_error: 0.5501\n",
      "Epoch 00036: val_loss did not improve from 0.45456\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2429 - mean_absolute_error: 0.5501 - val_loss: 0.4554 - val_mean_absolute_error: 0.8139\n",
      "Epoch 37/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2410 - mean_absolute_error: 0.5471\n",
      "Epoch 00037: val_loss did not improve from 0.45456\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2410 - mean_absolute_error: 0.5471 - val_loss: 0.4556 - val_mean_absolute_error: 0.8141\n",
      "Epoch 38/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2400 - mean_absolute_error: 0.5463\n",
      "Epoch 00038: val_loss improved from 0.45456 to 0.45247, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 33s 486us/sample - loss: 0.2400 - mean_absolute_error: 0.5462 - val_loss: 0.4525 - val_mean_absolute_error: 0.8100\n",
      "Epoch 39/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2388 - mean_absolute_error: 0.5444\n",
      "Epoch 00039: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2388 - mean_absolute_error: 0.5444 - val_loss: 0.4575 - val_mean_absolute_error: 0.8159\n",
      "Epoch 40/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2381 - mean_absolute_error: 0.5435\n",
      "Epoch 00040: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2382 - mean_absolute_error: 0.5435 - val_loss: 0.4554 - val_mean_absolute_error: 0.8136\n",
      "Epoch 41/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2369 - mean_absolute_error: 0.5420\n",
      "Epoch 00041: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2369 - mean_absolute_error: 0.5420 - val_loss: 0.4581 - val_mean_absolute_error: 0.8171\n",
      "Epoch 42/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2367 - mean_absolute_error: 0.5415\n",
      "Epoch 00042: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2367 - mean_absolute_error: 0.5415 - val_loss: 0.4552 - val_mean_absolute_error: 0.8134\n",
      "Epoch 43/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2360 - mean_absolute_error: 0.5406\n",
      "Epoch 00043: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2360 - mean_absolute_error: 0.5405 - val_loss: 0.4553 - val_mean_absolute_error: 0.8136\n",
      "Epoch 44/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2355 - mean_absolute_error: 0.5401\n",
      "Epoch 00044: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2355 - mean_absolute_error: 0.5401 - val_loss: 0.4536 - val_mean_absolute_error: 0.8112\n",
      "Epoch 45/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2348 - mean_absolute_error: 0.5388\n",
      "Epoch 00045: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2348 - mean_absolute_error: 0.5387 - val_loss: 0.4539 - val_mean_absolute_error: 0.8119\n",
      "Epoch 46/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2338 - mean_absolute_error: 0.5378\n",
      "Epoch 00046: val_loss did not improve from 0.45247\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2339 - mean_absolute_error: 0.5378 - val_loss: 0.4531 - val_mean_absolute_error: 0.8103\n",
      "Epoch 47/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2329 - mean_absolute_error: 0.5361\n",
      "Epoch 00047: val_loss improved from 0.45247 to 0.45096, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2329 - mean_absolute_error: 0.5360 - val_loss: 0.4510 - val_mean_absolute_error: 0.8081\n",
      "Epoch 48/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2324 - mean_absolute_error: 0.5355\n",
      "Epoch 00048: val_loss did not improve from 0.45096\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2324 - mean_absolute_error: 0.5354 - val_loss: 0.4540 - val_mean_absolute_error: 0.8114\n",
      "Epoch 49/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2322 - mean_absolute_error: 0.5351\n",
      "Epoch 00049: val_loss improved from 0.45096 to 0.45092, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 477us/sample - loss: 0.2322 - mean_absolute_error: 0.5351 - val_loss: 0.4509 - val_mean_absolute_error: 0.8080\n",
      "Epoch 50/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2320 - mean_absolute_error: 0.5346\n",
      "Epoch 00050: val_loss improved from 0.45092 to 0.45078, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2320 - mean_absolute_error: 0.5346 - val_loss: 0.4508 - val_mean_absolute_error: 0.8082\n",
      "Epoch 51/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2307 - mean_absolute_error: 0.5331\n",
      "Epoch 00051: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2307 - mean_absolute_error: 0.5331 - val_loss: 0.4520 - val_mean_absolute_error: 0.8092\n",
      "Epoch 52/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2310 - mean_absolute_error: 0.5332\n",
      "Epoch 00052: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2310 - mean_absolute_error: 0.5332 - val_loss: 0.4528 - val_mean_absolute_error: 0.8100\n",
      "Epoch 53/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2303 - mean_absolute_error: 0.5324\n",
      "Epoch 00053: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2303 - mean_absolute_error: 0.5323 - val_loss: 0.4511 - val_mean_absolute_error: 0.8080\n",
      "Epoch 54/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2292 - mean_absolute_error: 0.5306\n",
      "Epoch 00054: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2292 - mean_absolute_error: 0.5306 - val_loss: 0.4523 - val_mean_absolute_error: 0.8093\n",
      "Epoch 55/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2294 - mean_absolute_error: 0.5313\n",
      "Epoch 00055: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2294 - mean_absolute_error: 0.5313 - val_loss: 0.4510 - val_mean_absolute_error: 0.8080\n",
      "Epoch 56/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2287 - mean_absolute_error: 0.5299\n",
      "Epoch 00056: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2287 - mean_absolute_error: 0.5299 - val_loss: 0.4512 - val_mean_absolute_error: 0.8080\n",
      "Epoch 57/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2293 - mean_absolute_error: 0.5307\n",
      "Epoch 00057: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2293 - mean_absolute_error: 0.5307 - val_loss: 0.4518 - val_mean_absolute_error: 0.8086\n",
      "Epoch 58/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2286 - mean_absolute_error: 0.5298\n",
      "Epoch 00058: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2286 - mean_absolute_error: 0.5299 - val_loss: 0.4512 - val_mean_absolute_error: 0.8083\n",
      "Epoch 59/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2278 - mean_absolute_error: 0.5290\n",
      "Epoch 00059: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2279 - mean_absolute_error: 0.5291 - val_loss: 0.4521 - val_mean_absolute_error: 0.8092\n",
      "Epoch 60/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2273 - mean_absolute_error: 0.5276\n",
      "Epoch 00060: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2273 - mean_absolute_error: 0.5276 - val_loss: 0.4521 - val_mean_absolute_error: 0.8090\n",
      "Epoch 61/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2275 - mean_absolute_error: 0.5281\n",
      "Epoch 00061: val_loss did not improve from 0.45078\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2276 - mean_absolute_error: 0.5281 - val_loss: 0.4511 - val_mean_absolute_error: 0.8078\n",
      "Epoch 62/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2273 - mean_absolute_error: 0.5280\n",
      "Epoch 00062: val_loss improved from 0.45078 to 0.45051, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 477us/sample - loss: 0.2273 - mean_absolute_error: 0.5280 - val_loss: 0.4505 - val_mean_absolute_error: 0.8077\n",
      "Epoch 63/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2271 - mean_absolute_error: 0.5275\n",
      "Epoch 00063: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2271 - mean_absolute_error: 0.5275 - val_loss: 0.4506 - val_mean_absolute_error: 0.8070\n",
      "Epoch 64/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2264 - mean_absolute_error: 0.5269\n",
      "Epoch 00064: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2264 - mean_absolute_error: 0.5269 - val_loss: 0.4528 - val_mean_absolute_error: 0.8102\n",
      "Epoch 65/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2262 - mean_absolute_error: 0.5262\n",
      "Epoch 00065: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2262 - mean_absolute_error: 0.5262 - val_loss: 0.4543 - val_mean_absolute_error: 0.8115\n",
      "Epoch 66/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2260 - mean_absolute_error: 0.5259\n",
      "Epoch 00066: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2260 - mean_absolute_error: 0.5259 - val_loss: 0.4513 - val_mean_absolute_error: 0.8075\n",
      "Epoch 67/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2260 - mean_absolute_error: 0.5259\n",
      "Epoch 00067: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2259 - mean_absolute_error: 0.5259 - val_loss: 0.4527 - val_mean_absolute_error: 0.8092\n",
      "Epoch 68/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2253 - mean_absolute_error: 0.5250\n",
      "Epoch 00068: val_loss did not improve from 0.45051\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2253 - mean_absolute_error: 0.5249 - val_loss: 0.4520 - val_mean_absolute_error: 0.8087\n",
      "Epoch 69/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2253 - mean_absolute_error: 0.5251\n",
      "Epoch 00069: val_loss improved from 0.45051 to 0.45047, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2253 - mean_absolute_error: 0.5251 - val_loss: 0.4505 - val_mean_absolute_error: 0.8070\n",
      "Epoch 70/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2254 - mean_absolute_error: 0.5250\n",
      "Epoch 00070: val_loss improved from 0.45047 to 0.45025, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 478us/sample - loss: 0.2253 - mean_absolute_error: 0.5250 - val_loss: 0.4503 - val_mean_absolute_error: 0.8065\n",
      "Epoch 71/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2248 - mean_absolute_error: 0.5240\n",
      "Epoch 00071: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2247 - mean_absolute_error: 0.5240 - val_loss: 0.4520 - val_mean_absolute_error: 0.8086\n",
      "Epoch 72/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2246 - mean_absolute_error: 0.5239\n",
      "Epoch 00072: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2246 - mean_absolute_error: 0.5239 - val_loss: 0.4506 - val_mean_absolute_error: 0.8073\n",
      "Epoch 73/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2249 - mean_absolute_error: 0.5242\n",
      "Epoch 00073: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2248 - mean_absolute_error: 0.5242 - val_loss: 0.4520 - val_mean_absolute_error: 0.8086\n",
      "Epoch 74/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2243 - mean_absolute_error: 0.5232\n",
      "Epoch 00074: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2243 - mean_absolute_error: 0.5232 - val_loss: 0.4507 - val_mean_absolute_error: 0.8069\n",
      "Epoch 75/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2238 - mean_absolute_error: 0.5226\n",
      "Epoch 00075: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2238 - mean_absolute_error: 0.5226 - val_loss: 0.4525 - val_mean_absolute_error: 0.8092\n",
      "Epoch 76/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2239 - mean_absolute_error: 0.5230\n",
      "Epoch 00076: val_loss did not improve from 0.45025\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2239 - mean_absolute_error: 0.5230 - val_loss: 0.4513 - val_mean_absolute_error: 0.8078\n",
      "Epoch 77/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2234 - mean_absolute_error: 0.5222\n",
      "Epoch 00077: val_loss improved from 0.45025 to 0.44999, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2234 - mean_absolute_error: 0.5221 - val_loss: 0.4500 - val_mean_absolute_error: 0.8061\n",
      "Epoch 78/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2233 - mean_absolute_error: 0.5218\n",
      "Epoch 00078: val_loss did not improve from 0.44999\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2233 - mean_absolute_error: 0.5218 - val_loss: 0.4510 - val_mean_absolute_error: 0.8069\n",
      "Epoch 79/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2232 - mean_absolute_error: 0.5218\n",
      "Epoch 00079: val_loss did not improve from 0.44999\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2232 - mean_absolute_error: 0.5218 - val_loss: 0.4506 - val_mean_absolute_error: 0.8070\n",
      "Epoch 80/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2227 - mean_absolute_error: 0.5210\n",
      "Epoch 00080: val_loss improved from 0.44999 to 0.44982, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2227 - mean_absolute_error: 0.5210 - val_loss: 0.4498 - val_mean_absolute_error: 0.8059\n",
      "Epoch 81/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2229 - mean_absolute_error: 0.5214\n",
      "Epoch 00081: val_loss improved from 0.44982 to 0.44977, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2229 - mean_absolute_error: 0.5214 - val_loss: 0.4498 - val_mean_absolute_error: 0.8055\n",
      "Epoch 82/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2228 - mean_absolute_error: 0.5211\n",
      "Epoch 00082: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2229 - mean_absolute_error: 0.5212 - val_loss: 0.4524 - val_mean_absolute_error: 0.8090\n",
      "Epoch 83/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2222 - mean_absolute_error: 0.5206\n",
      "Epoch 00083: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2222 - mean_absolute_error: 0.5206 - val_loss: 0.4500 - val_mean_absolute_error: 0.8062\n",
      "Epoch 84/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2222 - mean_absolute_error: 0.5203\n",
      "Epoch 00084: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 470us/sample - loss: 0.2222 - mean_absolute_error: 0.5203 - val_loss: 0.4502 - val_mean_absolute_error: 0.8065\n",
      "Epoch 85/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2225 - mean_absolute_error: 0.5204\n",
      "Epoch 00085: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2225 - mean_absolute_error: 0.5204 - val_loss: 0.4515 - val_mean_absolute_error: 0.8077\n",
      "Epoch 86/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2224 - mean_absolute_error: 0.5205\n",
      "Epoch 00086: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2224 - mean_absolute_error: 0.5205 - val_loss: 0.4498 - val_mean_absolute_error: 0.8056\n",
      "Epoch 87/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2218 - mean_absolute_error: 0.5197\n",
      "Epoch 00087: val_loss did not improve from 0.44977\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2218 - mean_absolute_error: 0.5197 - val_loss: 0.4500 - val_mean_absolute_error: 0.8060\n",
      "Epoch 88/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2219 - mean_absolute_error: 0.5198\n",
      "Epoch 00088: val_loss improved from 0.44977 to 0.44898, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2219 - mean_absolute_error: 0.5197 - val_loss: 0.4490 - val_mean_absolute_error: 0.8045\n",
      "Epoch 89/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2219 - mean_absolute_error: 0.5196\n",
      "Epoch 00089: val_loss did not improve from 0.44898\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2218 - mean_absolute_error: 0.5196 - val_loss: 0.4498 - val_mean_absolute_error: 0.8054\n",
      "Epoch 90/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2215 - mean_absolute_error: 0.5192\n",
      "Epoch 00090: val_loss did not improve from 0.44898\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2215 - mean_absolute_error: 0.5191 - val_loss: 0.4527 - val_mean_absolute_error: 0.8096\n",
      "Epoch 91/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2215 - mean_absolute_error: 0.5193\n",
      "Epoch 00091: val_loss did not improve from 0.44898\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2215 - mean_absolute_error: 0.5193 - val_loss: 0.4522 - val_mean_absolute_error: 0.8084\n",
      "Epoch 92/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2220 - mean_absolute_error: 0.5197\n",
      "Epoch 00092: val_loss improved from 0.44898 to 0.44858, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2220 - mean_absolute_error: 0.5197 - val_loss: 0.4486 - val_mean_absolute_error: 0.8045\n",
      "Epoch 93/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2209 - mean_absolute_error: 0.5181\n",
      "Epoch 00093: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2209 - mean_absolute_error: 0.5181 - val_loss: 0.4494 - val_mean_absolute_error: 0.8054\n",
      "Epoch 94/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2215 - mean_absolute_error: 0.5191\n",
      "Epoch 00094: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2215 - mean_absolute_error: 0.5191 - val_loss: 0.4518 - val_mean_absolute_error: 0.8080\n",
      "Epoch 95/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2206 - mean_absolute_error: 0.5180\n",
      "Epoch 00095: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2207 - mean_absolute_error: 0.5181 - val_loss: 0.4510 - val_mean_absolute_error: 0.8073\n",
      "Epoch 96/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2207 - mean_absolute_error: 0.5179\n",
      "Epoch 00096: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2206 - mean_absolute_error: 0.5179 - val_loss: 0.4508 - val_mean_absolute_error: 0.8068\n",
      "Epoch 97/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2207 - mean_absolute_error: 0.5179\n",
      "Epoch 00097: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2206 - mean_absolute_error: 0.5179 - val_loss: 0.4490 - val_mean_absolute_error: 0.8049\n",
      "Epoch 98/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2209 - mean_absolute_error: 0.5182\n",
      "Epoch 00098: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2210 - mean_absolute_error: 0.5183 - val_loss: 0.4512 - val_mean_absolute_error: 0.8073\n",
      "Epoch 99/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2208 - mean_absolute_error: 0.5181\n",
      "Epoch 00099: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2208 - mean_absolute_error: 0.5181 - val_loss: 0.4508 - val_mean_absolute_error: 0.8071\n",
      "Epoch 100/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2206 - mean_absolute_error: 0.5176\n",
      "Epoch 00100: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2205 - mean_absolute_error: 0.5176 - val_loss: 0.4511 - val_mean_absolute_error: 0.8074\n",
      "Epoch 101/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2203 - mean_absolute_error: 0.5172\n",
      "Epoch 00101: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2204 - mean_absolute_error: 0.5174 - val_loss: 0.4511 - val_mean_absolute_error: 0.8070\n",
      "Epoch 102/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2202 - mean_absolute_error: 0.5168\n",
      "Epoch 00102: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2202 - mean_absolute_error: 0.5168 - val_loss: 0.4497 - val_mean_absolute_error: 0.8047\n",
      "Epoch 103/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2200 - mean_absolute_error: 0.5168\n",
      "Epoch 00103: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2200 - mean_absolute_error: 0.5167 - val_loss: 0.4509 - val_mean_absolute_error: 0.8062\n",
      "Epoch 104/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2201 - mean_absolute_error: 0.5170\n",
      "Epoch 00104: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2201 - mean_absolute_error: 0.5170 - val_loss: 0.4538 - val_mean_absolute_error: 0.8101\n",
      "Epoch 105/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2201 - mean_absolute_error: 0.5172\n",
      "Epoch 00105: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2200 - mean_absolute_error: 0.5172 - val_loss: 0.4488 - val_mean_absolute_error: 0.8044\n",
      "Epoch 106/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2199 - mean_absolute_error: 0.5168\n",
      "Epoch 00106: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2199 - mean_absolute_error: 0.5168 - val_loss: 0.4504 - val_mean_absolute_error: 0.8061\n",
      "Epoch 107/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2198 - mean_absolute_error: 0.5166\n",
      "Epoch 00107: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2198 - mean_absolute_error: 0.5166 - val_loss: 0.4515 - val_mean_absolute_error: 0.8078\n",
      "Epoch 108/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2198 - mean_absolute_error: 0.5163\n",
      "Epoch 00108: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2197 - mean_absolute_error: 0.5163 - val_loss: 0.4494 - val_mean_absolute_error: 0.8047\n",
      "Epoch 109/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2197 - mean_absolute_error: 0.5166\n",
      "Epoch 00109: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2197 - mean_absolute_error: 0.5166 - val_loss: 0.4509 - val_mean_absolute_error: 0.8069\n",
      "Epoch 110/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2195 - mean_absolute_error: 0.5164\n",
      "Epoch 00110: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2195 - mean_absolute_error: 0.5164 - val_loss: 0.4503 - val_mean_absolute_error: 0.8064\n",
      "Epoch 111/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2195 - mean_absolute_error: 0.5158\n",
      "Epoch 00111: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2195 - mean_absolute_error: 0.5158 - val_loss: 0.4533 - val_mean_absolute_error: 0.8099\n",
      "Epoch 112/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2199 - mean_absolute_error: 0.5164\n",
      "Epoch 00112: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2198 - mean_absolute_error: 0.5164 - val_loss: 0.4510 - val_mean_absolute_error: 0.8068\n",
      "Epoch 113/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2189 - mean_absolute_error: 0.5151\n",
      "Epoch 00113: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2189 - mean_absolute_error: 0.5152 - val_loss: 0.4512 - val_mean_absolute_error: 0.8075\n",
      "Epoch 114/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2187 - mean_absolute_error: 0.5151\n",
      "Epoch 00114: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2187 - mean_absolute_error: 0.5151 - val_loss: 0.4518 - val_mean_absolute_error: 0.8078\n",
      "Epoch 115/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2187 - mean_absolute_error: 0.5149\n",
      "Epoch 00115: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 470us/sample - loss: 0.2187 - mean_absolute_error: 0.5149 - val_loss: 0.4506 - val_mean_absolute_error: 0.8059\n",
      "Epoch 116/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2190 - mean_absolute_error: 0.5151\n",
      "Epoch 00116: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 468us/sample - loss: 0.2190 - mean_absolute_error: 0.5151 - val_loss: 0.4486 - val_mean_absolute_error: 0.8042\n",
      "Epoch 117/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2188 - mean_absolute_error: 0.5150\n",
      "Epoch 00117: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2188 - mean_absolute_error: 0.5150 - val_loss: 0.4514 - val_mean_absolute_error: 0.8078\n",
      "Epoch 118/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2188 - mean_absolute_error: 0.5150\n",
      "Epoch 00118: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2187 - mean_absolute_error: 0.5148 - val_loss: 0.4495 - val_mean_absolute_error: 0.8050\n",
      "Epoch 119/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2188 - mean_absolute_error: 0.5149\n",
      "Epoch 00119: val_loss did not improve from 0.44858\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2188 - mean_absolute_error: 0.5149 - val_loss: 0.4504 - val_mean_absolute_error: 0.8061\n",
      "Epoch 120/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2187 - mean_absolute_error: 0.5148\n",
      "Epoch 00120: val_loss improved from 0.44858 to 0.44845, saving model to results\\2021-11-30_MIXED-huber_loss-adam-LSTM-linear-layers-3-units-1000-b.h5\n",
      "67485/67485 [==============================] - 32s 477us/sample - loss: 0.2187 - mean_absolute_error: 0.5148 - val_loss: 0.4485 - val_mean_absolute_error: 0.8041\n",
      "Epoch 121/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2184 - mean_absolute_error: 0.5144\n",
      "Epoch 00121: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2184 - mean_absolute_error: 0.5144 - val_loss: 0.4519 - val_mean_absolute_error: 0.8078\n",
      "Epoch 122/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2188 - mean_absolute_error: 0.5147\n",
      "Epoch 00122: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2188 - mean_absolute_error: 0.5147 - val_loss: 0.4526 - val_mean_absolute_error: 0.8087\n",
      "Epoch 123/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2187 - mean_absolute_error: 0.5147\n",
      "Epoch 00123: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2188 - mean_absolute_error: 0.5148 - val_loss: 0.4505 - val_mean_absolute_error: 0.8064\n",
      "Epoch 124/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2185 - mean_absolute_error: 0.5144\n",
      "Epoch 00124: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2185 - mean_absolute_error: 0.5144 - val_loss: 0.4520 - val_mean_absolute_error: 0.8078\n",
      "Epoch 125/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2182 - mean_absolute_error: 0.5140\n",
      "Epoch 00125: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2182 - mean_absolute_error: 0.5140 - val_loss: 0.4509 - val_mean_absolute_error: 0.8070\n",
      "Epoch 126/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2186 - mean_absolute_error: 0.5142\n",
      "Epoch 00126: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2185 - mean_absolute_error: 0.5142 - val_loss: 0.4499 - val_mean_absolute_error: 0.8055\n",
      "Epoch 127/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2181 - mean_absolute_error: 0.5138\n",
      "Epoch 00127: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2181 - mean_absolute_error: 0.5138 - val_loss: 0.4531 - val_mean_absolute_error: 0.8094\n",
      "Epoch 128/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2180 - mean_absolute_error: 0.5139\n",
      "Epoch 00128: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2180 - mean_absolute_error: 0.5139 - val_loss: 0.4497 - val_mean_absolute_error: 0.8058\n",
      "Epoch 129/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2178 - mean_absolute_error: 0.5136\n",
      "Epoch 00129: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2179 - mean_absolute_error: 0.5137 - val_loss: 0.4497 - val_mean_absolute_error: 0.8052\n",
      "Epoch 130/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2182 - mean_absolute_error: 0.5138\n",
      "Epoch 00130: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2182 - mean_absolute_error: 0.5138 - val_loss: 0.4496 - val_mean_absolute_error: 0.8055\n",
      "Epoch 131/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2180 - mean_absolute_error: 0.5138\n",
      "Epoch 00131: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2180 - mean_absolute_error: 0.5138 - val_loss: 0.4525 - val_mean_absolute_error: 0.8090\n",
      "Epoch 132/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2182 - mean_absolute_error: 0.5139\n",
      "Epoch 00132: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2183 - mean_absolute_error: 0.5139 - val_loss: 0.4492 - val_mean_absolute_error: 0.8049\n",
      "Epoch 133/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2178 - mean_absolute_error: 0.5136\n",
      "Epoch 00133: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2177 - mean_absolute_error: 0.5136 - val_loss: 0.4511 - val_mean_absolute_error: 0.8071\n",
      "Epoch 134/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2176 - mean_absolute_error: 0.5130\n",
      "Epoch 00134: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2176 - mean_absolute_error: 0.5131 - val_loss: 0.4510 - val_mean_absolute_error: 0.8067\n",
      "Epoch 135/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2177 - mean_absolute_error: 0.5133\n",
      "Epoch 00135: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2176 - mean_absolute_error: 0.5133 - val_loss: 0.4496 - val_mean_absolute_error: 0.8051\n",
      "Epoch 136/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2179 - mean_absolute_error: 0.5136\n",
      "Epoch 00136: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2179 - mean_absolute_error: 0.5137 - val_loss: 0.4516 - val_mean_absolute_error: 0.8072\n",
      "Epoch 137/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2176 - mean_absolute_error: 0.5131\n",
      "Epoch 00137: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2177 - mean_absolute_error: 0.5131 - val_loss: 0.4497 - val_mean_absolute_error: 0.8052\n",
      "Epoch 138/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2178 - mean_absolute_error: 0.5135\n",
      "Epoch 00138: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2179 - mean_absolute_error: 0.5135 - val_loss: 0.4490 - val_mean_absolute_error: 0.8047\n",
      "Epoch 139/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2176 - mean_absolute_error: 0.5132\n",
      "Epoch 00139: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2176 - mean_absolute_error: 0.5132 - val_loss: 0.4532 - val_mean_absolute_error: 0.8093\n",
      "Epoch 140/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2174 - mean_absolute_error: 0.5127\n",
      "Epoch 00140: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2174 - mean_absolute_error: 0.5126 - val_loss: 0.4539 - val_mean_absolute_error: 0.8100\n",
      "Epoch 141/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2177 - mean_absolute_error: 0.5131\n",
      "Epoch 00141: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2177 - mean_absolute_error: 0.5131 - val_loss: 0.4513 - val_mean_absolute_error: 0.8068\n",
      "Epoch 142/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2175 - mean_absolute_error: 0.5129\n",
      "Epoch 00142: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2175 - mean_absolute_error: 0.5129 - val_loss: 0.4509 - val_mean_absolute_error: 0.8064\n",
      "Epoch 143/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2173 - mean_absolute_error: 0.5127\n",
      "Epoch 00143: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2173 - mean_absolute_error: 0.5127 - val_loss: 0.4519 - val_mean_absolute_error: 0.8075\n",
      "Epoch 144/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2176 - mean_absolute_error: 0.5129\n",
      "Epoch 00144: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2176 - mean_absolute_error: 0.5130 - val_loss: 0.4499 - val_mean_absolute_error: 0.8052\n",
      "Epoch 145/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2175 - mean_absolute_error: 0.5128\n",
      "Epoch 00145: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2175 - mean_absolute_error: 0.5128 - val_loss: 0.4532 - val_mean_absolute_error: 0.8092\n",
      "Epoch 146/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2171 - mean_absolute_error: 0.5122\n",
      "Epoch 00146: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2171 - mean_absolute_error: 0.5121 - val_loss: 0.4524 - val_mean_absolute_error: 0.8086\n",
      "Epoch 147/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2170 - mean_absolute_error: 0.5124\n",
      "Epoch 00147: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2170 - mean_absolute_error: 0.5124 - val_loss: 0.4523 - val_mean_absolute_error: 0.8079\n",
      "Epoch 148/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.5124\n",
      "Epoch 00148: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2172 - mean_absolute_error: 0.5124 - val_loss: 0.4518 - val_mean_absolute_error: 0.8074\n",
      "Epoch 149/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2171 - mean_absolute_error: 0.5120\n",
      "Epoch 00149: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2170 - mean_absolute_error: 0.5120 - val_loss: 0.4524 - val_mean_absolute_error: 0.8082\n",
      "Epoch 150/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.5127\n",
      "Epoch 00150: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2172 - mean_absolute_error: 0.5127 - val_loss: 0.4536 - val_mean_absolute_error: 0.8096\n",
      "Epoch 151/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2170 - mean_absolute_error: 0.5123\n",
      "Epoch 00151: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2170 - mean_absolute_error: 0.5122 - val_loss: 0.4486 - val_mean_absolute_error: 0.8037\n",
      "Epoch 152/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2172 - mean_absolute_error: 0.5121\n",
      "Epoch 00152: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 476us/sample - loss: 0.2172 - mean_absolute_error: 0.5120 - val_loss: 0.4518 - val_mean_absolute_error: 0.8075\n",
      "Epoch 153/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2169 - mean_absolute_error: 0.5120\n",
      "Epoch 00153: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2169 - mean_absolute_error: 0.5121 - val_loss: 0.4518 - val_mean_absolute_error: 0.8077\n",
      "Epoch 154/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.5118\n",
      "Epoch 00154: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2168 - mean_absolute_error: 0.5118 - val_loss: 0.4510 - val_mean_absolute_error: 0.8065\n",
      "Epoch 155/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.5120\n",
      "Epoch 00155: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2168 - mean_absolute_error: 0.5121 - val_loss: 0.4500 - val_mean_absolute_error: 0.8055\n",
      "Epoch 156/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2166 - mean_absolute_error: 0.5114\n",
      "Epoch 00156: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2166 - mean_absolute_error: 0.5114 - val_loss: 0.4509 - val_mean_absolute_error: 0.8067\n",
      "Epoch 157/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.5116\n",
      "Epoch 00157: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2167 - mean_absolute_error: 0.5116 - val_loss: 0.4494 - val_mean_absolute_error: 0.8044\n",
      "Epoch 158/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.5118\n",
      "Epoch 00158: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2168 - mean_absolute_error: 0.5117 - val_loss: 0.4547 - val_mean_absolute_error: 0.8106\n",
      "Epoch 159/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2170 - mean_absolute_error: 0.5119\n",
      "Epoch 00159: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2169 - mean_absolute_error: 0.5118 - val_loss: 0.4525 - val_mean_absolute_error: 0.8087\n",
      "Epoch 160/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.5118\n",
      "Epoch 00160: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2167 - mean_absolute_error: 0.5118 - val_loss: 0.4509 - val_mean_absolute_error: 0.8063\n",
      "Epoch 161/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2166 - mean_absolute_error: 0.5118\n",
      "Epoch 00161: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2166 - mean_absolute_error: 0.5117 - val_loss: 0.4530 - val_mean_absolute_error: 0.8092\n",
      "Epoch 162/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2164 - mean_absolute_error: 0.5110\n",
      "Epoch 00162: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2163 - mean_absolute_error: 0.5110 - val_loss: 0.4525 - val_mean_absolute_error: 0.8082\n",
      "Epoch 163/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2168 - mean_absolute_error: 0.5121\n",
      "Epoch 00163: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2168 - mean_absolute_error: 0.5121 - val_loss: 0.4530 - val_mean_absolute_error: 0.8088\n",
      "Epoch 164/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2164 - mean_absolute_error: 0.5114\n",
      "Epoch 00164: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2164 - mean_absolute_error: 0.5114 - val_loss: 0.4529 - val_mean_absolute_error: 0.8086\n",
      "Epoch 165/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2167 - mean_absolute_error: 0.5113\n",
      "Epoch 00165: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2167 - mean_absolute_error: 0.5113 - val_loss: 0.4531 - val_mean_absolute_error: 0.8089\n",
      "Epoch 166/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.5114\n",
      "Epoch 00166: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2165 - mean_absolute_error: 0.5114 - val_loss: 0.4532 - val_mean_absolute_error: 0.8090\n",
      "Epoch 167/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.5112\n",
      "Epoch 00167: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2165 - mean_absolute_error: 0.5112 - val_loss: 0.4523 - val_mean_absolute_error: 0.8080\n",
      "Epoch 168/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.5109\n",
      "Epoch 00168: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2161 - mean_absolute_error: 0.5109 - val_loss: 0.4523 - val_mean_absolute_error: 0.8080\n",
      "Epoch 169/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2162 - mean_absolute_error: 0.5109\n",
      "Epoch 00169: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2162 - mean_absolute_error: 0.5109 - val_loss: 0.4537 - val_mean_absolute_error: 0.8096\n",
      "Epoch 170/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2164 - mean_absolute_error: 0.5111\n",
      "Epoch 00170: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2164 - mean_absolute_error: 0.5110 - val_loss: 0.4521 - val_mean_absolute_error: 0.8075\n",
      "Epoch 171/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.5111\n",
      "Epoch 00171: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 474us/sample - loss: 0.2164 - mean_absolute_error: 0.5110 - val_loss: 0.4515 - val_mean_absolute_error: 0.8070\n",
      "Epoch 172/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.5105\n",
      "Epoch 00172: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2160 - mean_absolute_error: 0.5105 - val_loss: 0.4540 - val_mean_absolute_error: 0.8099\n",
      "Epoch 173/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2165 - mean_absolute_error: 0.5113\n",
      "Epoch 00173: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2164 - mean_absolute_error: 0.5112 - val_loss: 0.4558 - val_mean_absolute_error: 0.8119\n",
      "Epoch 174/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.5109\n",
      "Epoch 00174: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2162 - mean_absolute_error: 0.5109 - val_loss: 0.4509 - val_mean_absolute_error: 0.8059\n",
      "Epoch 175/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2159 - mean_absolute_error: 0.5103\n",
      "Epoch 00175: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2159 - mean_absolute_error: 0.5103 - val_loss: 0.4507 - val_mean_absolute_error: 0.8057\n",
      "Epoch 176/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.5105\n",
      "Epoch 00176: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2160 - mean_absolute_error: 0.5104 - val_loss: 0.4510 - val_mean_absolute_error: 0.8062\n",
      "Epoch 177/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.5106\n",
      "Epoch 00177: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2160 - mean_absolute_error: 0.5107 - val_loss: 0.4520 - val_mean_absolute_error: 0.8073\n",
      "Epoch 178/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2157 - mean_absolute_error: 0.5101\n",
      "Epoch 00178: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2157 - mean_absolute_error: 0.5102 - val_loss: 0.4540 - val_mean_absolute_error: 0.8097\n",
      "Epoch 179/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.5101\n",
      "Epoch 00179: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2159 - mean_absolute_error: 0.5101 - val_loss: 0.4528 - val_mean_absolute_error: 0.8084\n",
      "Epoch 180/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2161 - mean_absolute_error: 0.5105\n",
      "Epoch 00180: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2160 - mean_absolute_error: 0.5105 - val_loss: 0.4554 - val_mean_absolute_error: 0.8119\n",
      "Epoch 181/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2162 - mean_absolute_error: 0.5107\n",
      "Epoch 00181: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2161 - mean_absolute_error: 0.5106 - val_loss: 0.4529 - val_mean_absolute_error: 0.8088\n",
      "Epoch 182/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2159 - mean_absolute_error: 0.5106\n",
      "Epoch 00182: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2159 - mean_absolute_error: 0.5106 - val_loss: 0.4557 - val_mean_absolute_error: 0.8120\n",
      "Epoch 183/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.5102\n",
      "Epoch 00183: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2158 - mean_absolute_error: 0.5101 - val_loss: 0.4522 - val_mean_absolute_error: 0.8078\n",
      "Epoch 184/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2159 - mean_absolute_error: 0.5105\n",
      "Epoch 00184: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2159 - mean_absolute_error: 0.5105 - val_loss: 0.4541 - val_mean_absolute_error: 0.8104\n",
      "Epoch 185/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.5102\n",
      "Epoch 00185: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2159 - mean_absolute_error: 0.5104 - val_loss: 0.4535 - val_mean_absolute_error: 0.8098\n",
      "Epoch 186/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.5105\n",
      "Epoch 00186: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2160 - mean_absolute_error: 0.5105 - val_loss: 0.4547 - val_mean_absolute_error: 0.8109\n",
      "Epoch 187/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.5102\n",
      "Epoch 00187: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2158 - mean_absolute_error: 0.5102 - val_loss: 0.4518 - val_mean_absolute_error: 0.8074\n",
      "Epoch 188/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2160 - mean_absolute_error: 0.5103\n",
      "Epoch 00188: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2160 - mean_absolute_error: 0.5103 - val_loss: 0.4529 - val_mean_absolute_error: 0.8084\n",
      "Epoch 189/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2156 - mean_absolute_error: 0.5098\n",
      "Epoch 00189: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2156 - mean_absolute_error: 0.5098 - val_loss: 0.4517 - val_mean_absolute_error: 0.8075\n",
      "Epoch 190/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2155 - mean_absolute_error: 0.5099\n",
      "Epoch 00190: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2156 - mean_absolute_error: 0.5099 - val_loss: 0.4544 - val_mean_absolute_error: 0.8108\n",
      "Epoch 191/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2159 - mean_absolute_error: 0.5103\n",
      "Epoch 00191: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2159 - mean_absolute_error: 0.5102 - val_loss: 0.4558 - val_mean_absolute_error: 0.8125\n",
      "Epoch 192/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2158 - mean_absolute_error: 0.5104\n",
      "Epoch 00192: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 475us/sample - loss: 0.2158 - mean_absolute_error: 0.5104 - val_loss: 0.4532 - val_mean_absolute_error: 0.8095\n",
      "Epoch 193/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2157 - mean_absolute_error: 0.5099\n",
      "Epoch 00193: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2157 - mean_absolute_error: 0.5099 - val_loss: 0.4514 - val_mean_absolute_error: 0.8073\n",
      "Epoch 194/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2155 - mean_absolute_error: 0.5098\n",
      "Epoch 00194: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2156 - mean_absolute_error: 0.5099 - val_loss: 0.4528 - val_mean_absolute_error: 0.8089\n",
      "Epoch 195/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2156 - mean_absolute_error: 0.5101\n",
      "Epoch 00195: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2156 - mean_absolute_error: 0.5101 - val_loss: 0.4522 - val_mean_absolute_error: 0.8082\n",
      "Epoch 196/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2155 - mean_absolute_error: 0.5097\n",
      "Epoch 00196: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2154 - mean_absolute_error: 0.5097 - val_loss: 0.4544 - val_mean_absolute_error: 0.8106\n",
      "Epoch 197/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2154 - mean_absolute_error: 0.5099\n",
      "Epoch 00197: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2154 - mean_absolute_error: 0.5098 - val_loss: 0.4526 - val_mean_absolute_error: 0.8086\n",
      "Epoch 198/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2155 - mean_absolute_error: 0.5097\n",
      "Epoch 00198: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 471us/sample - loss: 0.2155 - mean_absolute_error: 0.5097 - val_loss: 0.4542 - val_mean_absolute_error: 0.8102\n",
      "Epoch 199/200\n",
      "67392/67485 [============================>.] - ETA: 0s - loss: 0.2152 - mean_absolute_error: 0.5093\n",
      "Epoch 00199: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 472us/sample - loss: 0.2152 - mean_absolute_error: 0.5093 - val_loss: 0.4519 - val_mean_absolute_error: 0.8075\n",
      "Epoch 200/200\n",
      "67456/67485 [============================>.] - ETA: 0s - loss: 0.2151 - mean_absolute_error: 0.5091\n",
      "Epoch 00200: val_loss did not improve from 0.44845\n",
      "67485/67485 [==============================] - 32s 473us/sample - loss: 0.2151 - mean_absolute_error: 0.5091 - val_loss: 0.4537 - val_mean_absolute_error: 0.8097\n"
     ]
    }
   ],
   "source": [
    "#def run_tensorflow():\n",
    "\n",
    "# create these folders if they does not exist\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 72\n",
    "# Lookup step, 1 is the next day\n",
    "#LOOKUP_STEP = int(run_dict[run][\"LOOKUP_STEP\"])\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.3\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"close_0\",\"ema_0\",\"high_0\",\"low_0\",\"open_0\",\"rsi_0\",\"sma_0\",\"volume_0\",\"close_1\",\"ema_1\",\"high_1\",\"low_1\",\"open_1\",\"rsi_1\",\"sma_1\",\"volume_1\",\n",
    "                   \"close_2\",\"ema_2\",\"high_2\",\"low_2\",\"open_2\",\"rsi_2\",\"sma_2\",\"volume_2\",\"close_3\",\"ema_3\",\"high_3\",\"low_3\",\"open_3\",\"rsi_3\",\"sma_3\",\"volume_3\",\n",
    "                   \"close_4\",\"ema_4\",\"high_4\",\"low_4\",\"open_4\",\"rsi_4\",\"sma_4\",\"volume_4\",\"close_5\",\"ema_5\",\"high_5\",\"low_5\",\"open_5\",\"rsi_5\",\"sma_5\",\"volume_5\",\n",
    "                   \"close_6\",\"ema_6\",\"high_6\",\"low_6\",\"open_6\",\"rsi_6\",\"sma_6\",\"volume_6\",\"close_7\",\"ema_7\",\"high_7\",\"low_7\",\"open_7\",\"rsi_7\",\"sma_7\",\"volume_7\",\n",
    "                   \"close_8\",\"ema_8\",\"high_8\",\"low_8\",\"open_8\",\"rsi_8\",\"sma_8\",\"volume_8\"]\n",
    "TARGET_COLUMNS = [\"close_9\",\"high_9\",\"low_9\",\"open_9\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 1000\n",
    "# 40% dropout\n",
    "DROPOUT = 0.25\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "LAYER_ACTIVATION = \"exponential\"\n",
    "\n",
    "# Stock market\n",
    "ticker = \"MIXED\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-{LAYER_ACTIVATION}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#try:\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/processed/all_processed_10.csv')\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL, layer_activation=LAYER_ACTIVATION)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "X = data[FEATURE_COLUMNS]\n",
    "y = data[TARGET_COLUMNS]\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# reshape X to fit the neural network\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")\n",
    "\n",
    "#except:\n",
    "#    print(\"There was an attempt.\")\n",
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
